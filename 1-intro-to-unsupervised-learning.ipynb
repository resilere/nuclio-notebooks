{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\\n",
    "# <center><span style=\"background-color: rgb(251, 160, 38); font-size: 32px;\">  <b>NUCLIO DIGITAL SCHOOL -</b> MASTER EN DATA SCIENCE  </span></center>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center><a href = https://nuclio.school/wp-content/uploads/2019/10/nucleoDS-newBlack.png > <img src=\"https://nuclio.school/wp-content/uploads/2019/10/nucleoDS-newBlack.png\" width=400 height=100><a/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">\n",
    "<center> <span style=\"font-size: 26px;\">  Introduction to Unupervised Learning </span> </center>\n",
    "\n",
    "+ Session: **Introduction to Unsupervised Learning**\n",
    "+ Module: **Unsupervised Learning**\n",
    "+ Course: **Data Science Master 0921**\n",
    "+ Professor: **Christa Santos**\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have already explained in the Slides, distance is a key aspect that we will use to measure the similarity between the samples. The samples can be represented in coordinates through the value of their variables, and the distance between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](img\\distance_metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use two-dimensional data, and four-dimensional data to see that they can be applied regardless of the number of variables used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [0,1]\n",
    "b = [4,5]\n",
    "c = [4,5]\n",
    "A = [0,1,3]\n",
    "B = [4,5,6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the calculation is done through simple formulas, we will directly use the scipy library that allows us to use the formulas directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean distance\n",
    "It is defined as the minimum distance between two points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Two dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(distance.euclidean(a,b).round(2))\n",
    "display(distance.euclidean(b,c).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(distance.euclidean(A,B).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance.euclidean([2, 0, 0,7], [0, 2, 0,3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan distance\n",
    "The Manhattan distance is the sum of the absolute differences between the points of all dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(distance.cityblock(a,b).round(2))\n",
    "display(distance.cityblock(b,c).round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(distance.cityblock(A,B).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski distance\n",
    "The Minkowski distance is the generalized form of the Euclidean distance and the Manhattan distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(distance.minkowski(a,b).round(2))\n",
    "display(distance.minkowski(b,c).round(2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(distance.minkowski(A,B).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity\n",
    "Mathematically, it measures the cosine of the angle between two vectors projected in a multidimensional space. The cosine similarity captures the orientation (angle) of the difference and not the magnitude. Therefore, it is a judgment of orientation and not of magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors oriented at 90 ° to each other have a similarity of 0, and two diametrically opposite vectors have a similarity of - 1, regardless of its magnitude. The cosine similarity is used particularly in the positive space, where the result is clearly delimited in [0,1]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(distance.cosine(a,b).round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(distance.cosine(b,c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(distance.cosine(A,B).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Scaler\n",
    "\n",
    "StandardScaler removes the mean and scales each characteristic / variable to the unit of variance. \n",
    "\n",
    "This operation is carried out independently for each variable in the sample.\n",
    "\n",
    "StandardScaler can be affected by outliers (if the data set exists) since it implies the estimation of the empirical mean and the standard deviation of each variable.\n",
    "\n",
    "Standardize a characteristic by subtracting the mean and then scaling to the unit variance. Unit variance means dividing all values ​​by the standard deviation. \n",
    "\n",
    "StandardScaler does not meet the strict definition of scale that was introduced earlier. The result is a distribution with a standard deviation equal to 1. The variance is equal to 1 as well, because variance = standard deviation squared and 1 squared is 1.\n",
    "\n",
    "StandardScaler makes the mean of the distribution 0. About 68% of the values ​​will be between -1 and 1.\n",
    "\n",
    "StandardScaler distorts the relative distances between feature values, so it's generally my second choice in this family of transforms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "X = np.array([[0, 0], [1, 0], [0, 1], [1, 1]])\n",
    "# the scaler object (model)\n",
    "scaler = StandardScaler()\n",
    "# fit and transform the data\n",
    "scaled_data = scaler.fit_transform(X) \n",
    "\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_data.mean(axis = 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MinMax Scaler\n",
    "\n",
    "For each value of a variable, MinMaxScaler subtracts the minimum value of the variable and then divides it by the range. \n",
    "\n",
    "The range is the difference between the original maximum and minimum.\n",
    "\n",
    "MinMaxScaler preserves the shape of the original layout. \n",
    "\n",
    "It does not significantly change the information in the original data.\n",
    "\n",
    "Also, MinMaxScaler does not reduce the importance of outliers.\n",
    "\n",
    "The default range for the characteristic returned by MinMaxScaler is 0 to 1.\n",
    "\n",
    "MinMaxScaler is a good algorithm to start with except when you require your feature to have a normal distribution or you want to reduce the importance of outliers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler,RobustScaler\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "print(X.shape)\n",
    "# (150, 4) # 150 samples (rows) with 4 features/variables (columns)\n",
    "# build the scaler model\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit using the train set\n",
    "scaler.fit(X)\n",
    "# transform the test test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = scaler.transform(X)\n",
    "# Verify minimum value of all features\n",
    "X_scaled.min(axis=0)\n",
    "# array([0., 0., 0., 0.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify maximum value of all features\n",
    "X_scaled.max(axis=0)\n",
    "# array([1., 1., 1., 1.])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 8\n",
    "#plt.style.use('seaborn-darkgrid')\n",
    "fig, axes = plt.subplots(1,2)\n",
    "axes[0].scatter(X[:,0], X[:,1], c=y)\n",
    "axes[0].set_title(\"Original data\")\n",
    "axes[1].scatter(X_scaled[:,0], X_scaled[:,1], c=y)\n",
    "axes[1].set_title(\"MinMax scaled data\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare all methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "  \n",
    "# data\n",
    "x = pd.DataFrame({\n",
    "    # Distribution with lower outliers\n",
    "    'x1': np.concatenate([np.random.normal(20, 2, 1000), np.random.normal(1, 2, 25)]),\n",
    "    # Distribution with higher outliers\n",
    "    'x2': np.concatenate([np.random.normal(30, 2, 1000), np.random.normal(50, 2, 25)]),\n",
    "})\n",
    "np.random.normal\n",
    "  \n",
    "scaler = preprocessing.RobustScaler()\n",
    "robust_df = scaler.fit_transform(x)\n",
    "robust_df = pd.DataFrame(robust_df, columns =['x1', 'x2'])\n",
    "  \n",
    "scaler = preprocessing.StandardScaler()\n",
    "standard_df = scaler.fit_transform(x)\n",
    "standard_df = pd.DataFrame(standard_df, columns =['x1', 'x2'])\n",
    "  \n",
    "scaler = preprocessing.MinMaxScaler()\n",
    "minmax_df = scaler.fit_transform(x)\n",
    "minmax_df = pd.DataFrame(minmax_df, columns =['x1', 'x2'])\n",
    "  \n",
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(ncols = 4, figsize =(20, 5))\n",
    "ax1.set_title('Before Scaling')\n",
    "  \n",
    "sns.kdeplot(x['x1'], ax = ax1, color ='r')\n",
    "sns.kdeplot(x['x2'], ax = ax1, color ='b')\n",
    "ax2.set_title('After Robust Scaling')\n",
    "  \n",
    "sns.kdeplot(robust_df['x1'], ax = ax2, color ='red')\n",
    "sns.kdeplot(robust_df['x2'], ax = ax2, color ='blue')\n",
    "ax3.set_title('After Standard Scaling')\n",
    "  \n",
    "sns.kdeplot(standard_df['x1'], ax = ax3, color ='red')\n",
    "sns.kdeplot(standard_df['x2'], ax = ax3, color ='blue')\n",
    "ax4.set_title('After Min-Max Scaling')\n",
    "  \n",
    "sns.kdeplot(minmax_df['x1'], ax = ax4, color ='red')\n",
    "sns.kdeplot(minmax_df['x2'], ax = ax4, color ='blue')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instance Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html), gives us a lot of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process Outline\n",
    "\n",
    "In the example shown below, the following steps are performed:\n",
    "\n",
    "0. The k-nearest neighbor algorithm from the scikit-learn package is imported.\n",
    "1. The dataset is loaded and a first exploration of the data is made\n",
    "2. The target is defined and the variables for the model are chosen\n",
    "3. Variables are scaled before applying the method\n",
    "4. Divide the data into training and test data.\n",
    "5. Generate a k-NN model using the value of the neighbors.\n",
    "6. Train or fit the data to the model.\n",
    "7. Evaluate the model\n",
    "8. Predict the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "# Loading data\n",
    "irisData = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisData.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(irisData.data).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose the variables for the model and the target. In this case it is simple, we want to predict the categorical variable through the numerical ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature and target arrays\n",
    "X = irisData.data\n",
    "y = irisData.target\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the data. As in all models we partition the data, in train and test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "             X, y, test_size = 0.2, random_state=42)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create the instance with the number of neighbors to use. We will see this problem later, so we choose a specific number, 7 in this case, and create the instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we train the model. We use our train dataset to shape the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's evaluate the metrics. We evaluate our metric in two different ways: with the predict. Visualization can also be done in the case of two dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting new values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy:\", metrics.accuracy_score(y_test,knn.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.predict([[3,2,3,4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example with Validation - Ideal Number of Neighbors\n",
    "\n",
    "To calculate the ideal number of neighbors, we use a validation data set, and we run the algorithm with a different number of neighbors. Then we select the optimal one using the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import necessary modules\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split into training and test set\n",
    "X_train_def, X_val, y_train_def, y_val = train_test_split(\n",
    "             X_train, y_train, test_size = 0.2, random_state=46)\n",
    "  \n",
    "neighbors = np.arange(1, 9)\n",
    "train_accuracy = np.empty(len(neighbors))\n",
    "test_accuracy = np.empty(len(neighbors))\n",
    "  \n",
    "# Loop over K values\n",
    "for i, k in enumerate(neighbors):\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train_def, y_train_def)\n",
    "      \n",
    "    # Compute traning and test data accuracy\n",
    "    train_accuracy[i] = knn.score(X_train_def, y_train_def)\n",
    "    test_accuracy[i] = knn.score(X_val, y_val)\n",
    "  \n",
    "# Generate plot\n",
    "plt.plot(neighbors, test_accuracy, label = 'Validation dataset Accuracy')\n",
    "plt.plot(neighbors, train_accuracy, label = 'Training dataset Accuracy')\n",
    "  \n",
    "plt.legend()\n",
    "plt.xlabel('n_neighbors')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the calculation is redone with the new number of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Advantages of applying KNN\n",
    "+ **Easy Training**: The training phase of the K-nearest neighbor classification is much faster compared to other classification algorithms. You do not need to train a model for generalization, which is why KNN is known as the simple, instance-based learning algorithm.\n",
    "+ **Ability to work with different data**: KNN can be useful in case of non-linear data.\n",
    "+ **Can be used for regression**: Can be used with regression problem. The output value of the object is calculated by the mean of the value of the k closest neighbors.\n",
    "\n",
    "Disadvantages of the model\n",
    "+ **Computationally expensive**: The closest K-next classification testing phase is slower and more expensive in terms of time and memory. It requires a large amount of memory to store the entire set of training data for prediction.\n",
    "+ **Need for Normalization**: KNN requires scaling the data because KNN uses the Euclidean distance between two data points to find the closest neighbors. The Euclidean distance is sensitive to magnitudes. Features with high magnitudes will have more weight than features with low magnitudes.\n",
    "+ **Large Dimensions**: KNN is also not suitable for large data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Excessive dimensions\n",
    "\n",
    "Next, we will see a slightly absurd example to see that an excess of variables does not significantly improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"As the number of features or dimensions grows, the amount of data we need to generalize accurately grows exponentially.\" -  Charles Isbell, Professor and Senior Associate Dean, School of Interactive Computing, Georgia Tech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from time import time\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=3000, n_features=1500, n_classes=2, n_informative=2, random_state=43)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time()\n",
    "reg = LogisticRegression()\n",
    " \n",
    "reg.fit(X_train, y_train)\n",
    "y_pred = reg.predict(X_test)\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "memory1 = pd.DataFrame(X_train).memory_usage().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=100).fit(X_train)\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    " \n",
    "start = time()\n",
    "reg.fit(X_train_pca, y_train)\n",
    "y_pred1 = reg.predict(X_test_pca)\n",
    "print(f'Accuracy:',accuracy_score(y_test, y_pred1))\n",
    "print(f'Time:',time() - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory2 = pd.DataFrame(X_train_pca).memory_usage().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory1/memory2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your turn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the species of each penguin using the instances algorithm (and all numerical variables). Caution about NAs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_penguins = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv'\n",
    "df_penguins = pd.read_csv(link_penguins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguins.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_penguins.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
