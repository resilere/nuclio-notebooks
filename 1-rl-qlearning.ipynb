{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Nuclio logo](https://nuclio.school/wp-content/uploads/2018/12/nucleoDS-newBlack.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU9exTKDReNb"
      },
      "source": [
        "# Reinforced Learning with Q-learning - The autonomous taxi\n",
        "\n",
        "The first example of reinforced learning more serious than the multi-armed bandit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74-GeJIoRYlr"
      },
      "source": [
        "## 1. Let's install the libraries in Colab to have a gym and be able to show the recorded videos\n",
        "More details in this sample notebook from the Colab team https://colab.research.google.com/drive/18LdlDDT87eb8cCTHZsXyS9ksQPzL3i6H#scrollTo=7wY4qZhPXotR"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "griqzz1cRiyw",
        "outputId": "f16f31ab-db8d-47e3-f290-df3483580e59"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "Suggested packages:\n",
            "  libgle3\n",
            "The following NEW packages will be installed:\n",
            "  python-opengl\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 496 kB of archives.\n",
            "After this operation, 5,416 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/universe amd64 python-opengl all 3.1.0+dfsg-1 [496 kB]\n",
            "Fetched 496 kB in 1s (681 kB/s)\n",
            "Selecting previously unselected package python-opengl.\n",
            "(Reading database ... 155229 files and directories currently installed.)\n",
            "Preparing to unpack .../python-opengl_3.1.0+dfsg-1_all.deb ...\n",
            "Unpacking python-opengl (3.1.0+dfsg-1) ...\n",
            "Setting up python-opengl (3.1.0+dfsg-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following NEW packages will be installed:\n",
            "  xvfb\n",
            "0 upgraded, 1 newly installed, 0 to remove and 37 not upgraded.\n",
            "Need to get 784 kB of archives.\n",
            "After this operation, 2,271 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n",
            "Fetched 784 kB in 1s (1,006 kB/s)\n",
            "Selecting previously unselected package xvfb.\n",
            "(Reading database ... 157584 files and directories currently installed.)\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n"
          ]
        }
      ],
      "source": [
        "!pip install gym\n",
        "!apt-get install python-opengl -y\n",
        "!apt install xvfb -y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IPjF4AN8Rpdq",
        "outputId": "c659c331-d196-41a9-f7b4-d674b0acb067"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gym[atari] in /usr/local/lib/python3.7/dist-packages (0.17.3)\n",
            "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.19.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.4.1)\n",
            "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (1.5.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (4.1.2.30)\n",
            "Requirement already satisfied: atari-py~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (0.2.9)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from gym[atari]) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from atari-py~=0.2.0->gym[atari]) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym[atari]) (0.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install gym[atari]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9NHOvZTRy7D"
      },
      "source": [
        "For rendering environment, you can use pyvirtualdisplay. So fulfill that "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3aLPLs5RsT_",
        "outputId": "bda0921e-b09c-4896-c244-b199bf9b1528"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-2.2-py3-none-any.whl (15 kB)\n",
            "Collecting EasyProcess\n",
            "  Downloading EasyProcess-0.3-py2.py3-none-any.whl (7.9 kB)\n",
            "Installing collected packages: EasyProcess, pyvirtualdisplay\n",
            "Successfully installed EasyProcess-0.3 pyvirtualdisplay-2.2\n",
            "Collecting piglet\n",
            "  Downloading piglet-1.0.0-py2.py3-none-any.whl (2.2 kB)\n",
            "Collecting piglet-templates\n",
            "  Downloading piglet_templates-1.2.0-py3-none-any.whl (67 kB)\n",
            "\u001b[K     |████████████████████████████████| 67 kB 2.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (21.4.0)\n",
            "Requirement already satisfied: markupsafe in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (2.0.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (3.0.6)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.7/dist-packages (from piglet-templates->piglet) (1.6.3)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (1.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse->piglet-templates->piglet) (0.37.1)\n",
            "Installing collected packages: piglet-templates, piglet\n",
            "Successfully installed piglet-1.0.0 piglet-templates-1.2.0\n"
          ]
        }
      ],
      "source": [
        "!pip install pyvirtualdisplay\n",
        "!pip install piglet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbjJFCguR0T_",
        "outputId": "9362baa3-985c-4d36-ed7b-698f7f9e6da8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<pyvirtualdisplay.display.Display at 0x7f76bef3ca90>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from pyvirtualdisplay import Display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "7ha6sY7xR6Yl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "if type(os.environ.get(\"DISPLAY\")) is not str or len(os.environ.get(\"DISPLAY\"))==0:\n",
        "    !bash ../xvfb start\n",
        "    %env DISPLAY=:1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3CetumuVR8d8"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import logger as gymlogger\n",
        "from gym.wrappers import Monitor\n",
        "gymlogger.set_level(40) # error only\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import math\n",
        "import glob\n",
        "import io\n",
        "import base64\n",
        "from IPython.display import HTML\n",
        "\n",
        "from IPython import display as ipythondisplay"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Plzal7qjR_dL"
      },
      "source": [
        "The following two functions are to be able to video record the gym environments and show them\n",
        "\n",
        "To activate the video, you just have to do \"env = wrap_env(env)\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "GGeBd6NZR_Lu"
      },
      "outputs": [],
      "source": [
        "def show_video():\n",
        "  mp4list = glob.glob('video/*.mp4')\n",
        "  if len(mp4list) > 0:\n",
        "    mp4 = mp4list[0]\n",
        "    video = io.open(mp4, 'r+b').read()\n",
        "    encoded = base64.b64encode(video)\n",
        "    ipythondisplay.display(HTML(data='''<video alt=\"test\" autoplay \n",
        "                loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "             </video>'''.format(encoded.decode('ascii'))))\n",
        "  else: \n",
        "    print(\"Could not find video\")\n",
        "    \n",
        "\n",
        "def wrap_env(env):\n",
        "  env = Monitor(env, './video', force=True)\n",
        "  return env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6g46cRfmSFSN"
      },
      "source": [
        "## 2. The Taxi environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dw6ymd7pSKcO",
        "outputId": "05f1a7dd-820c-4a79-970f-e260d2a781ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|\u001b[43mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env = gym.make(\"Taxi-v3\").env\n",
        "\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I11HSgQDSVaZ"
      },
      "source": [
        "This looks a bit like what we have been seeing in the slides, the only difference (which was like this in the v2 version of Taxi) is that the taxi is in position R, instead of (1,3) as we had seen before"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZ7FY6lCSdZQ"
      },
      "source": [
        "The universe of the gym interface is <code> environment </code>. We have some methods that we will be using that may be useful to you:\n",
        "\n",
        "<ul>\n",
        "   <li> <code> environment.reset </code> Puts the environment in factory position, and returns a random initial state </li>\n",
        "   <li> <code> environment.step (action) </code> Step of the environment after a time increment. This returns us:\n",
        "   <ul>\n",
        "     <li> <b> observation </b>: Observations of the environment </li>\n",
        "     <li> <b> reward </b>: The reward we collect </li>\n",
        "     <li> <b> done </b>: indicates if we have successfully picked up or dropped off a passenger, which we will call an <i> episode </i> </li>\n",
        "     <li> <b> info </b>: Additional information such as performance or latency for debugging </li>\n",
        "   </ul>\n",
        "   <li> <code> environment.render </code> Paint a frame of the environment (very useful to get an idea of it)\n",
        "\n",
        "Additional note: we have made <code> gym.make (\"Taxi-v3\").<b>env</b> </code> to avoid stopping us at 200 iterations which is how Gym works by default"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux7z0asDSpcc"
      },
      "source": [
        "### Let's remember the problem\n",
        "\n",
        "We have 4 locations (with different letters), and our job is to pick up a passenger at one of them and take him to another. We receive 20 points for a successful delivery, and we lose 1 point for each time step (to optimize the journey). We have incorporated a 10 point penalty for illegal deliveries or pickups at the wrong locations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XVjcvGQEScvD",
        "outputId": "a7cdeb56-14cb-421d-b3d0-e52c4776f1e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|R: | : :G|\n",
            "| : | : : |\n",
            "| :\u001b[43m \u001b[0m: : : |\n",
            "| | : | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "env.reset()\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YI11Bz_GSyCi"
      },
      "source": [
        "Notice that now the taxi is in another location.\n",
        "\n",
        "More things from the environment:\n",
        "* The yellow box is the taxi\n",
        "* The walls or hedges where there is no road are the pipes (|)\n",
        "* R, G, Y, B are the pick-up and drop-off points. The color <font color = \"blue\"> <b> blue </b> </font> indicates a collection point, the color <font color = \"purple\"> <b> purple </b> </font> indicates delivery point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBoVulzKSYJ2",
        "outputId": "6e6b5a56-f166-48d9-a069-2e7b08a0e97d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Action space Discrete(6)\n",
            "State space Discrete(500)\n"
          ]
        }
      ],
      "source": [
        "print(\"Action space {}\".format(env.action_space))\n",
        "print(\"State space {}\".format(env.observation_space))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vpo5EWayTBdb"
      },
      "source": [
        "Notice that the Action Space is of size 6 and the State Space is of size 500.\n",
        "\n",
        "In terms of mapping that you know that the actions go from 0 to 5 with these values:\n",
        "\n",
        "* 0 = south\n",
        "* 1 = north\n",
        "* 2 = east\n",
        "* 3 = west\n",
        "* 4 = pick up passenger\n",
        "* 5 = leave passenger\n",
        "\n",
        "Index of collection or delivery points\n",
        "\n",
        "* 0 = R (0,0)\n",
        "* 1 = G (0.4)\n",
        "* 2 = Y (4,0)\n",
        "* 3 = B (4,3)\n",
        "\n",
        "But first of all, we are going to take a look at that position (3,1) that we had seen, we will see the information of that state. With a passenger waiting in position 2 (<font color = \"blue\"> <b> Y </b> </font>) with destination 0 (<font color = \"purple\"> <b> R </b> </font>)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0aiLd86xTBOK",
        "outputId": "f4d15143-b5a0-4512-923a-be2429e3879e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "State: 328\n",
            "+---------+\n",
            "|\u001b[35mR\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| |\u001b[43m \u001b[0m: | : |\n",
            "|\u001b[34;1mY\u001b[0m| : |B: |\n",
            "+---------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "state = env.encode(3, 1, 2, 0) # (taxi row, taxi column, passenger position index, destination position index)\n",
        "print(\"State:\", state)\n",
        "\n",
        "env.s = state # Podemos asignar el estado actual al que queramos definir\n",
        "env.render()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWSAYUzcTXoE"
      },
      "source": [
        "### The reward table\n",
        "\n",
        "As you can see from 0 to 499 coordinates that we have in our environment, it would be interesting if what we have arranged in our rewards in the presentation is the same as there is in Gym"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vIOzktNVTcrh",
        "outputId": "665c18b9-69a8-4058-ac8e-e8e5eb49dc35"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{0: [(1.0, 428, -1, False)],\n",
              " 1: [(1.0, 228, -1, False)],\n",
              " 2: [(1.0, 348, -1, False)],\n",
              " 3: [(1.0, 328, -1, False)],\n",
              " 4: [(1.0, 328, -10, False)],\n",
              " 5: [(1.0, 328, -10, False)]}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "env.P[328]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgg620n4TkBF"
      },
      "source": [
        "The indices in the dictionary at position 328 are the actions to take from this point. And they follow a very clear structure:\n",
        "\n",
        "<code> {action: [(probability, next state, reward, done)]} </code>\n",
        "\n",
        "As you can see, in this environment we have assigned a <code> probability </code> to each action of 100% (we do not make distinctions, nor do we force agent behavior.\n",
        "\n",
        "<code> next state </code> tells us in which coordinates we would end up if we took the action of the index.\n",
        "\n",
        "The <code> reward </code> is shown in that third position, with values ​​of -1 if we \"add\" one more time step and -10 if the taxi happens to pick up or drop off a passenger. And if we looked at the correct destination coordinate with a passenger inside the taxi, a nice 20 would appear in <code> reward </code> in action 5 (leave passenger).\n",
        "\n",
        "The <code> performed </code> position is used to tell us that we have dropped a passenger at the correct location. Each successful installment will be our **episode** finale."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igfZUtzLTsFU"
      },
      "source": [
        "## 3. What if we let the taxi do it alone (no Reinforcement Learning)\n",
        "\n",
        "By the method of brute force. It is about using the <code> P </code> table of rewards, which will be the guide for navigating the taxi.\n",
        "\n",
        "The idea is to set up an infinite loop that won't stop until the taxi drops a passenger at their destination (a simple **episode**). Or when we receive a reward of 20.\n",
        "\n",
        "The <code> environment.action_space.sample () </code> method takes an action randomly from all possible actions.\n",
        "\n",
        "Let's see what happens ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUfy4y9xTxQ4",
        "outputId": "fc0889f7-02c5-4901-e2a9-34bef558f388"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Time steps used: 1342\n",
            "Accumulated penalties: 436\n",
            "Results after 1 episodes:\n",
            "Average time per episode: 1342.0\n",
            "Average of punishments per episode: 436.0\n"
          ]
        }
      ],
      "source": [
        "env.s = 328\n",
        "\n",
        "epochs = 0\n",
        "punishment, reward = 0, 0\n",
        "\n",
        "total_epochs = 0\n",
        "total_punishments = 0\n",
        "\n",
        "frames = [] # for animation !!\n",
        "\n",
        "terminal_episode = False\n",
        "\n",
        "while not terminal_episode:\n",
        "    action = env.action_space.sample()\n",
        "    state, reward, terminal_episode, info = env.step(action)\n",
        "\n",
        "    if reward == -10:\n",
        "      punishment += 1\n",
        "    \n",
        "    # We put each rendered frame inside a dictionary for animation\n",
        "    frames.append ({\n",
        "        'frame': env.render(mode = 'ansi'),\n",
        "        'episode': epochs,\n",
        "        'state': state,\n",
        "        'action': action,\n",
        "        'reward': reward\n",
        "        }\n",
        "    )\n",
        "    epochs += 1\n",
        "\n",
        "total_punishments += punishment\n",
        "total_epochs += epochs\n",
        "episodes = 1\n",
        "    \n",
        "print (\"Time steps used: {}\". format(epochs))\n",
        "print (\"Accumulated penalties: {}\". format(punishment))\n",
        "\n",
        "random_halftime = total_epochs / episodes\n",
        "average_random_punishment = total_punishments / episodes\n",
        "\n",
        "print (f\"Results after {episodes} episodes:\")\n",
        "print (f\"Average time per episode: {random_halftime}\")\n",
        "print (f\"Average of punishments per episode: {average_random_punishment}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-Zr9QeMU1NY"
      },
      "source": [
        "### Let's format the results in a nice animation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4z3cIccU7NM",
        "outputId": "7fbc7879-ca4b-429b-ca95-be707d8cb07f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+---------+\n",
            "|\u001b[35m\u001b[34;1m\u001b[43mR\u001b[0m\u001b[0m\u001b[0m: | : :G|\n",
            "| : | : : |\n",
            "| : : : : |\n",
            "| | : | : |\n",
            "|Y| : |B: |\n",
            "+---------+\n",
            "  (Dropoff)\n",
            "\n",
            "Episode: 1341\n",
            "Time: 1342\n",
            "State: 0\n",
            "Action: 5\n",
            "Reward: 20\n"
          ]
        }
      ],
      "source": [
        "from IPython.display import clear_output\n",
        "from time import sleep\n",
        "\n",
        "def visualise_frames(frames):\n",
        "    for i, frame in enumerate(frames):\n",
        "        clear_output(wait=True)\n",
        "        print(frame['frame'])\n",
        "        print(f\"Episode: {frame['episode']}\")\n",
        "        print(f\"Time: {i + 1}\")\n",
        "        print(f\"State: {frame['state']}\")\n",
        "        print(f\"Action: {frame['action']}\")\n",
        "        print(f\"Reward: {frame['reward']}\")\n",
        "        sleep(.1)\n",
        "\n",
        "visualise_frames(frames)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cwkrmw_VW_B"
      },
      "source": [
        "It has cost him his life to deliver the passenger to his destination ... not bad."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R45SpRaGVai2"
      },
      "source": [
        "## 4. Q-Learning's turn\n",
        "\n",
        "Let us remember that now we must update the values of Q (a, s) in the Q-table, because it is from that guide that we will get the best actions for our agent, the taxi.\n",
        "\n",
        "Higher or better values of Q will indicate where we have to go with our taxi. In other words, if the taxi is in a state where a passenger is waiting for it, it is quite likely that the values for <code> pick up passenger </code> are higher than the rest of the available actions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60VD4zmCVzKt"
      },
      "source": [
        "### 4.1 Initializing the Q-table to a 500x6 matrix filled with zeros"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_kFDHaqV4WJ",
        "outputId": "40d12f57-e6f3-44b6-9d65-7ad09849cbf4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(500, 6)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
        "\n",
        "print(q_table.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8JuaJymlV-N9"
      },
      "source": [
        "We set the time, and we load a few necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcGNUX6wV_DZ",
        "outputId": "8c6c71ff-9d1c-42bc-fe05-3808d1fa2c8a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CPU times: user 10 µs, sys: 2 µs, total: 12 µs\n",
            "Wall time: 16.5 µs\n"
          ]
        }
      ],
      "source": [
        "%%time\n",
        "\n",
        "from IPython.display import clear_output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlc-TtTMWF7t"
      },
      "source": [
        "### 4.2 Let's build the code for training\n",
        "\n",
        "We will define the update parameters of the Q table (alpha, gamma) and the epsilon ... yes our greedy friend epsilon!!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "WHu5ZNPqWFQo"
      },
      "outputs": [],
      "source": [
        "alpha = 0.1 # Our learning rate\n",
        "gamma = 0.6 # Our reward discount\n",
        "epsilon = 0.1 # greedy epsilon"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QrsIlmyIWP_9"
      },
      "source": [
        "We define a variable to be able to measure metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "tdV_PIcrWP0G"
      },
      "outputs": [],
      "source": [
        "all_epochs = []\n",
        "all_penalties = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIJAyy8RWaBz"
      },
      "source": [
        "We set up the episode loop, where inside we will put the episode loop itself."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FLj8G-O2Wa_Z",
        "outputId": "23892388-ed61-4d45-d582-99f94c6d8262"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode: 100000\n",
            "Training finished. \\ n\n"
          ]
        }
      ],
      "source": [
        "for i in range (1, 100001):\n",
        "  state = env.reset () # We start the environment in a random position each time\n",
        "\n",
        "  epochs, punishment, reward = 0, 0, 0\n",
        " \n",
        "  terminal_episode = False\n",
        "\n",
        "  while not terminal_episode:\n",
        "    # Here goes our friend Epsilon greedy, to determine when we explore or explode\n",
        "    if np.random.random () < epsilon:\n",
        "      action = env.action_space.sample() # We explore the action space\n",
        "    else:\n",
        "      action = np.argmax(q_table[state]) # We exploit what we have learned\n",
        "    \n",
        "    next_state, reward, terminal_episode, info = env.step (action)\n",
        "\n",
        "    # Let's update the values ​​of the Q table in the action position, state after seeing what happened to us\n",
        "    # We store the previous value of table Q\n",
        "    old_value = q_table[state, action]\n",
        "\n",
        "    # From the states position, I keep the value of the action that would give the highest value in the Q table\n",
        "    next_max = np.max(q_table[next_state])\n",
        "\n",
        "    # We calculate the update form for Q (a, s)\n",
        "    new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
        "    \n",
        "    q_table[state, action] = new_value\n",
        "\n",
        "    if reward == -10:\n",
        "      punishment += 1\n",
        "\n",
        "    state = next_state\n",
        "    epochs += 1\n",
        "\n",
        "  if i% 100 == 0:\n",
        "    clear_output (wait = True)\n",
        "    print (f\"Episode: {i}\")\n",
        "\n",
        "\n",
        "\n",
        "print (\"Training finished. \\ n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12Ui52bhW9WB"
      },
      "source": [
        "After the training is finished, let's look at the values of position 328 for each action available in that environment as learned by the taxi."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK4m1jdCXBFj",
        "outputId": "89b398df-c57e-473c-e97f-5c45d0a948e8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ -2.39618228,  -2.27325184,  -2.40921663,  -2.3577794 ,\n",
              "       -10.60114447, -10.95515178])"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "q_table[328]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwCtg4ahXDuN"
      },
      "source": [
        "Action 1 (go north) is the one with the best \"reward\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZwAF93nXMKe"
      },
      "source": [
        "### 4.3 Let's evaluate how our agent does it\n",
        "\n",
        "Now we don't explore anymore, so we remove the Epsilon Greedy part and use directly from the Q-table values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H9wWwPERXEt1"
      },
      "outputs": [],
      "source": [
        "total_epochs, total_punishments = 0, 0\n",
        "episodes = 100\n",
        "\n",
        "frames = [] # for animation\n",
        "\n",
        "for i in range (episodes):\n",
        "    state = env.reset ()\n",
        "    epochs, punishment, reward = 0, 0, 0\n",
        "    \n",
        "    terminal_episode = False\n",
        "    \n",
        "    while not terminal_episode:\n",
        "        action = np.argmax(q_table[state])\n",
        "        status, reward, terminal_episode, info = env.step(action)\n",
        "\n",
        "        if reward == -10:\n",
        "          punishment += 1\n",
        "        \n",
        "        epochs += 1\n",
        "\n",
        "        # We put each rendered frame inside a dictionary for animation\n",
        "        frames.append ({\n",
        "          'frame': env.render(mode = 'ansi'),\n",
        "          'episode': i,\n",
        "          'state': state,\n",
        "          'action': action,\n",
        "          'reward': reward\n",
        "          }\n",
        "        )\n",
        "\n",
        "    total_punishments += punishment\n",
        "    total_epochs += epochs\n",
        "\n",
        "qlearning_time = total_epochs / episodes\n",
        "average_punishment_qlearning = total_punishments / episodes\n",
        "\n",
        "print (f\"Results after {episodes} episodes:\")\n",
        "print (f\"Average time per episode: {qlearning_time}\")\n",
        "print (f\"Average punishments per episode: {average_punishment_qlearning}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3yeQ8AfxXwWe"
      },
      "outputs": [],
      "source": [
        "visualise_frames(frames)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "1-rl-qlearning.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
