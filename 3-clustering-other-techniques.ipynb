{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><span style=\"background-color: rgb(251, 160, 38); font-size: 32px;\">  <b>NUCLIO DIGITAL SCHOOL -</b> MASTER EN DATA SCIENCE  </span></center>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center><a href = https://nuclio.school/wp-content/uploads/2019/10/nucleoDS-newBlack.png > <img src=\"https://nuclio.school/wp-content/uploads/2019/10/nucleoDS-newBlack.png\" width=400 height=100><a/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">\n",
    "    <center> <span style=\"font-size: 26px;\"> Clustering Techniques </span> </center>\n",
    "\n",
    "+ Session: **Clustering Techniques**\n",
    "+ Module: **Unsupervised Learning**\n",
    "+ Course: **Data Science Master 0921**\n",
    "+ Professor: **Christa Santos**\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affinity Propagation\n",
    "\n",
    "Affinity Propagation involves finding a set of exemplars that best summarize the data.\n",
    "\n",
    "Clustering data by identifying a subset of representative examples is important for processing sensory signals and detecting patterns in data. Such “exemplars” can be found by randomly choosing an initial subset of data points and then iteratively refining it, but this works well only if that initial choice is close to a good solution. We devised a method called “affinity propagation,” which takes as input measures of similarity between pairs of data points. Real-valued messages are exchanged between data points until a high-quality set of exemplars and corresponding clusters gradually emerges. We used affinity propagation to cluster images of faces, detect genes in microarray data, identify representative sentences in this manuscript, and identify cities that are efficiently accessed by airline travel. Affinity propagation found clusters with much lower error than other methods, and it did so in less than one-hundredth the amount of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# affinity propagation clustering\n",
    "from numpy import unique\n",
    "from numpy import where\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from matplotlib import pyplot as plt\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 15, 8\n",
    "plt.style.use('seaborn-darkgrid')\n",
    "# define dataset\n",
    "X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
    "# define the model\n",
    "model = AffinityPropagation(damping=0.9)\n",
    "# fit the model\n",
    "model.fit(X)\n",
    "# assign a cluster to each example\n",
    "yhat = model.predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\trow_ix = where(yhat == cluster)\n",
    "\t# create scatter of these samples\n",
    "\tplt.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agglomerative Clustering\n",
    "\n",
    "Agglomerative clustering involves merging examples until the desired number of clusters is achieved.\n",
    "\n",
    "Agglomerative hierarchical clustering differs from k-means in a key way. Rather than choosing a number of clusters and starting out with random centroids, we instead begin with every point in our dataset as a “cluster.” Then we find the two closest points and combine them into a cluster. Then, we find the next closest points, and those become a cluster. We repeat the process until we only have one big giant cluster.\n",
    "\n",
    "Along the way, we create what’s called a dendrogram. This is our “history.” You can see the dendrogram for our data points below to get a sense of what’s happening.\n",
    "\n",
    "![dendogram](https://scikit-learn.org/stable/_images/sphx_glr_plot_agglomerative_dendrogram_001.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# agglomerative clustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "# define dataset\n",
    "X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
    "# define the model\n",
    "model = AgglomerativeClustering(n_clusters=2)\n",
    "# fit model and predict clusters\n",
    "yhat = model.fit_predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\trow_ix = where(yhat == cluster)\n",
    "\t# create scatter of these samples\n",
    "\tplt.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BIRCH\n",
    "\n",
    "BIRCH Clustering (BIRCH is short for Balanced Iterative Reducing and Clustering using Hierarchies) involves constructing a tree structure from which cluster centroids are extracted.\n",
    "\n",
    "BIRCH incrementally and dynamically clusters incoming multi-dimensional metric data points to try to produce the best quality clustering with the available resources (i.e., available memory and time constraints). BIRCH can typically find a good clustering with a single scan of the data, and improve the quality further with a few additional scans. BIRCH is also the first clustering algorithm proposed in the database area to handle \"noise\" (data points that are not part of the underlying pattern) effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# birch clustering\n",
    "from sklearn.cluster import Birch\n",
    "# define dataset\n",
    "X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
    "# define the model\n",
    "model = Birch(threshold=0.01, n_clusters=2)\n",
    "# fit the model\n",
    "model.fit(X)\n",
    "# assign a cluster to each example\n",
    "yhat = model.predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\trow_ix = where(yhat == cluster)\n",
    "\t# create scatter of these samples\n",
    "\tplt.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DBSCAN\n",
    "\n",
    "DBSCAN Clustering (where DBSCAN is short for Density-Based Spatial Clustering of Applications with Noise) involves finding high-density areas in the domain and expanding those areas of the feature space around them as clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dbscan clustering\n",
    "from sklearn.cluster import DBSCAN\n",
    "# define dataset\n",
    "X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
    "# define the model\n",
    "model = DBSCAN(eps=0.30, min_samples=9)\n",
    "# fit model and predict clusters\n",
    "yhat = model.fit_predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\trow_ix = where(yhat == cluster)\n",
    "\t# create scatter of these samples\n",
    "\tplt.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means\n",
    "\n",
    "K-Means Clustering may be the most widely known clustering algorithm and involves assigning examples to clusters in an effort to minimize the variance within each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# k-means clustering\n",
    "from sklearn.cluster import KMeans\n",
    "# define dataset\n",
    "X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
    "# define the model\n",
    "model = KMeans(n_clusters=2)\n",
    "# fit the model\n",
    "model.fit(X)\n",
    "# assign a cluster to each example\n",
    "yhat = model.predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\trow_ix = where(yhat == cluster)\n",
    "\t# create scatter of these samples\n",
    "\tplt.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mini-Batch K-Means\n",
    "\n",
    "Mini-Batch K-Means is a modified version of k-means that makes updates to the cluster centroids using mini-batches of samples rather than the entire dataset, which can make it faster for large datasets, and perhaps more robust to statistical noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mini-batch k-means clustering\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "# define dataset\n",
    "X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
    "# define the model\n",
    "model = MiniBatchKMeans(n_clusters=2)\n",
    "# fit the model\n",
    "model.fit(X)\n",
    "# assign a cluster to each example\n",
    "yhat = model.predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\trow_ix = where(yhat == cluster)\n",
    "\t# create scatter of these samples\n",
    "\tplt.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mean Shift\n",
    "\n",
    "Mean shift clustering involves finding and adapting centroids based on the density of examples in the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean shift clustering\n",
    "from sklearn.cluster import MeanShift\n",
    "# define dataset\n",
    "X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
    "# define the model\n",
    "model = MeanShift()\n",
    "# fit model and predict clusters\n",
    "yhat = model.fit_predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\trow_ix = where(yhat == cluster)\n",
    "\t# create scatter of these samples\n",
    "\tplt.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OPTICS\n",
    "\n",
    "OPTICS clustering (where OPTICS is short for Ordering Points To Identify the Clustering Structure) is a modified version of DBSCAN described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optics clustering\n",
    "from sklearn.cluster import OPTICS\n",
    "# define dataset\n",
    "X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
    "# define the model\n",
    "model = OPTICS(eps=0.8, min_samples=10)\n",
    "# fit model and predict clusters\n",
    "yhat = model.fit_predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\trow_ix = where(yhat == cluster)\n",
    "\t# create scatter of these samples\n",
    "\tplt.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spectral Clustering\n",
    "Spectral Clustering is a general class of clustering methods, drawn from linear algebra."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spectral clustering\n",
    "from sklearn.cluster import SpectralClustering\n",
    "# define dataset\n",
    "X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
    "# define the model\n",
    "model = SpectralClustering(n_clusters=2)\n",
    "# fit model and predict clusters\n",
    "yhat = model.fit_predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\trow_ix = where(yhat == cluster)\n",
    "\t# create scatter of these samples\n",
    "\tplt.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Mixture Model\n",
    "A Gaussian mixture model summarizes a multivariate probability density function with a mixture of Gaussian probability distributions as its name suggests.\n",
    "\n",
    "In statistics, a mixture model is a probabilistic model for representing the presence of subpopulations within an overall population, without requiring that an observed data set should identify the sub-population to which an individual observation belongs. Formally a mixture model corresponds to the mixture distribution that represents the probability distribution of observations in the overall population. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gaussian mixture clustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "# define dataset\n",
    "X, _ = make_classification(n_samples=1000, n_features=2, n_informative=2, n_redundant=0, n_clusters_per_class=1, random_state=4)\n",
    "# define the model\n",
    "model = GaussianMixture(n_components=2)\n",
    "# fit the model\n",
    "model.fit(X)\n",
    "# assign a cluster to each example\n",
    "yhat = model.predict(X)\n",
    "# retrieve unique clusters\n",
    "clusters = unique(yhat)\n",
    "# create scatter plot for samples from each cluster\n",
    "for cluster in clusters:\n",
    "\t# get row indexes for samples with this cluster\n",
    "\trow_ix = where(yhat == cluster)\n",
    "\t# create scatter of these samples\n",
    "\tplt.scatter(X[row_ix, 0], X[row_ix, 1])\n",
    "# show the plot\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0820050dde91fc2897eae9036e8ba4d22e350a3297f4d4b49abc97f4c24f3b88"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
