{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><span style=\"background-color: rgb(251, 160, 38); font-size: 32px;\">  <b>NUCLIO DIGITAL SCHOOL -</b> MASTER EN DATA SCIENCE  </span></center>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center><a href = https://nuclio.school/wp-content/uploads/2019/10/nucleoDS-newBlack.png > <img src=\"https://nuclio.school/wp-content/uploads/2019/10/nucleoDS-newBlack.png\" width=400 height=100><a/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">\n",
    "    <center> <span style=\"font-size: 26px;\"> KMeans with pipelines </span> </center>\n",
    "\n",
    "+ Session: **KMeans with pipelines**\n",
    "+ Module: **Unsupervised Learning**\n",
    "+ Course: **Data Science Master 0921**\n",
    "+ Professor: **Christa Santos**\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"toc\"></a>\n",
    "## Table of Contents\n",
    "[1. Objective](#objectives)\n",
    "\n",
    "[2. Import the main libraries](#import_modules)\n",
    "\n",
    "[3. Import the data](#import_data)\n",
    "\n",
    "[4. Exploratory Data Analysis (EDA)](#eda)\n",
    "\n",
    "---> [4.1 Customers Dataset](#df1)\n",
    "\n",
    "---> [4.2 Orders Dataset](#df2)\n",
    "\n",
    "---> [4.3 Payments Dataset](#df3)\n",
    "\n",
    "[5. Variable Creation](#variables)\n",
    "\n",
    "[6. Final Dataset Creation](#join)\n",
    "\n",
    "[7. Pipeline Creation](#skpipeline)\n",
    "\n",
    "[8. Elbow curve](#elbow_curve)\n",
    "\n",
    "[9. Customer segmentation with the \"appropriate k\"](#segmentation)\n",
    "\n",
    "[10. RFM Model](#rfm)\n",
    "\n",
    "[11. Summary](#summary)\n",
    "\n",
    "[12. Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"objectives\"></a>\n",
    "## 1. Objective\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "In this notebook we are going to work with the dataset of the **Brazilian E-Commerce company called Olist.**\n",
    "\n",
    "The company operates with a philosophy of ***SaaS (Software as a Service)***. Its basic product is a marketplace where stores with or without an online presence can sell their products to customers who visit their website.\n",
    "\n",
    "![Data Model](https://i.imgur.com/HRhd2Y0.png)\n",
    "\n",
    "Using the Olista dataset (around 100 thousand records) we are going to build a segmentation based on the KMeans algorithm and with a focus on constructing ***RFM variables (recency - frequency - monetary value).*** This It is a very common way of working in startups and online stores where little customer information is available and the retention / exchange of clicks is essential.\n",
    "\n",
    "Our main objectives will be:\n",
    "1. **Build business variables (based on segmentation)** from 3 different datasets and put them together correctly so as not to generate duplicate records.\n",
    "\n",
    "2. **Learn to use the Sklearn Pipeline** to greatly speed up the transformation of the dataset and the creation of variables.\n",
    "\n",
    "3. **Learn to implement our own Transformers** that can be used inside the Pipelines (for example to eliminate outliers).\n",
    "\n",
    "4. Use the elbow diagram to determine the **\"optimal\"** number of centroids for the KMeans model.\n",
    "\n",
    "5. **Summarize the information of our clusters in a more friendly format** using Pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"import_modules\"></a>\n",
    "## 2. Import the main libraries\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "In this section of the kernel we are going to load the main libraries that we are going to use in our notebook during the implementation of the **KMeans algorithm**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silence warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# operating system\n",
    "import os\n",
    "\n",
    "# time calculation to track some processes\n",
    "import time\n",
    "\n",
    "# numeric and matrix operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# loading ploting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# python core library for machine learning and data science\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "RANDOM_STATE = 175\n",
    "PATH_FOLDER = os.path.join(os.getcwd(), \"data\\\\olist_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"import_data\"></a>\n",
    "## 3. Import the data\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "In this section of the kernel we are going to load the main datasets that we are going to use for our segmentation.\n",
    "\n",
    "You can **add other datasets or external data** to drill down or experiment with the KMeans algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_CUSTOMERS = os.path.join(PATH_FOLDER, 'olist_customers_dataset.csv')\n",
    "\n",
    "customer_df = pd.read_csv(PATH_CUSTOMERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ORDERS = os.path.join(PATH_FOLDER, 'olist_orders_dataset.csv')\n",
    "\n",
    "orders_df = pd.read_csv(PATH_ORDERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PAYMENTS = os.path.join(PATH_FOLDER, 'olist_order_payments_dataset.csv')\n",
    "\n",
    "payments_df = pd.read_csv(PATH_PAYMENTS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"eda\"></a>\n",
    "## 4. Exploratory Data Analysis (EDA)\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "In the EDA section we will make **a first approximation to our data** to see its composition and what variables we have at our disposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"df1\"></a>\n",
    "### 4.1 Customers Dataset\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "Quick EDA on the **customer dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_df(df, verbose = True):\n",
    "    '''\n",
    "    Makes a simple report on the supplied DataFrame.\n",
    "    '''\n",
    "    print(df.info(verbose = verbose))\n",
    "    total_nulls = df.isnull().sum().sum()\n",
    "    print()\n",
    "    print(f\"We have a total of {total_nulls} nulls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df(customer_df)\n",
    "\n",
    "customer_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_id = customer_df[\"customer_id\"].nunique()\n",
    "customer_unique_id = customer_df[\"customer_unique_id\"].nunique()\n",
    "\n",
    "print(f'''We have a total of {customer_unique_id} unique customers, for a total of {customer_id} orders (in Olist's schema, they specify that the unique id is customer_unique_id and that customer_id is an id that is generated in each purchase and therefore in practice it is the same as an order).\n",
    "This implies a ratio of {round(customer_id/customer_unique_id, 2)} orders per customer.\n",
    "''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a groupby DataFrame by customer city\n",
    "city_pivot_ = customer_df.groupby([\"customer_state\", \"customer_city\"]).size()\\\n",
    ".sort_values(ascending = False).to_frame().reset_index().rename(columns = {0: \"clients_per_city\"})\n",
    "\n",
    "city_pivot_[\"cumsum_by_city\"] = (city_pivot_[\"clients_per_city\"]/city_pivot_[\"clients_per_city\"].sum()).cumsum()\n",
    "\n",
    "city_pivot_[\"pct_cities\"] = (1/city_pivot_.shape[0])\n",
    "city_pivot_[\"pct_cities\"] = city_pivot_[\"pct_cities\"].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a groupby DataFrame by customer state\n",
    "state_pivot_ = customer_df.groupby(\"customer_state\").size()\\\n",
    ".sort_values(ascending = False).to_frame().reset_index().rename(columns = {0: \"clients_per_state\"})\n",
    "\n",
    "state_pivot_[\"cumsum_by_state\"] = (state_pivot_[\"clients_per_state\"]/state_pivot_[\"clients_per_state\"].sum()).cumsum()\n",
    "\n",
    "state_pivot_[\"pct_state\"] = (1/state_pivot_.shape[0])\n",
    "state_pivot_[\"pct_state\"] = state_pivot_[\"pct_state\"].cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------\n",
    "# Plotting part\n",
    "\n",
    "# Instantiate the figure\n",
    "fig = plt.figure(figsize = (10, 15))\n",
    "ax1, ax2, ax3 = fig.subplots(nrows = 3, ncols = 1)\n",
    "\n",
    "# get the data\n",
    "# concate a zero before the list, so that all curves start at origin\n",
    "x1_values = [0] + list(city_pivot_.index)\n",
    "y1_values = [0] + list(city_pivot_[\"cumsum_by_city\"])\n",
    "\n",
    "x2_values = [0] + list(state_pivot_[\"cumsum_by_state\"])\n",
    "y2_values = [0] + list(state_pivot_[\"cumsum_by_state\"])\n",
    "\n",
    "x3_values_city = [0] + list(city_pivot_[\"pct_cities\"])\n",
    "y3_values_city = [0] + list(city_pivot_[\"cumsum_by_city\"])\n",
    "\n",
    "x3_values_state = [0] + list(state_pivot_[\"pct_state\"])\n",
    "y3_values_state = [0] + list(state_pivot_[\"cumsum_by_state\"])\n",
    "\n",
    "# plot the values and set for every subplot a title\n",
    "ax1.plot(y1_values)\n",
    "ax1.title.set_text(\"Percentage of Accumulated Customers by City\")\n",
    "\n",
    "ax2.plot(y2_values, color = \"green\", alpha = 0.5)\n",
    "ax2.title.set_text(\"Percentage of Accumulated Customers by State\")\n",
    "\n",
    "ax3.plot(x3_values_city, y3_values_city, label = \"Percentage of Accumulated Customers by City\")\n",
    "ax3.plot(x3_values_state, y3_values_state, label = \"Percentage of Accumulated Customers by State\", \n",
    "         color = \"green\", alpha = 0.5)\n",
    "ax3.title.set_text(\"Percentage of Accumulated Customers by State and City\")\n",
    "\n",
    "ax3.legend()\n",
    "\n",
    "# create a title for the figure\n",
    "fig.suptitle('Cumulative distribution of customers by City and State (absolute and relative)', fontsize = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"df2\"></a>\n",
    "### 4.2 Orders Dataset\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "Quick EDA on the **order dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df(orders_df)\n",
    "\n",
    "orders_df.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df[\"order_status\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have nulls in the dates, we will either have to eliminate these nulls or assigning a date to them.\n",
    "orders_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_id = orders_df[\"order_id\"].nunique()\n",
    "customer_id_orders = orders_df[\"customer_id\"].nunique()\n",
    "\n",
    "# We make sure that all orders have their corresponding customer in the customers table\n",
    "assert (set(orders_df[\"customer_id\"]) ^ set(customer_df[\"customer_id\"])) == set()\n",
    "assert (set(customer_df[\"customer_id\"]) ^ set(orders_df[\"customer_id\"])) == set()\n",
    "\n",
    "print(f\"We have a total of {order_id} orders\")\n",
    "print(f\"We have a total of {customer_unique_id} unique customers (from the customer dataset)\")\n",
    "print(f\"The relation of orders by clients is of {round(order_id/customer_unique_id, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"df3\"></a>\n",
    "### 4.3 Payments Dataset\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "Quick EDA on the **payment dataset.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df(payments_df)\n",
    "\n",
    "payments_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# payment sequential: a customer may pay an order with more than one payment method. If he does so, a sequence will be created to accommodate all payments.\n",
    "\n",
    "payments_df['payment_sequential'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "order_id_pay = payments_df[\"order_id\"].nunique()\n",
    "print(f\"We have a total of {order_id_pay} unique orders in the payments table.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make sure that all orders have their corresponding customer in the customers table\n",
    "len(set(orders_df[\"order_id\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(orders_df[\"order_id\"]) ^ set(payments_df[\"order_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df[orders_df[\"order_id\"] == \"bfbd0f9bdef84302105ad712db648a6c\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"variables\"></a>\n",
    "## Feature Engineering\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "We have analyzed the 3 key datasets with which we are going to work.\n",
    "\n",
    "In this section of the notebook, **we will add the dataset and we will generate new variables** so that later in the next section we will put it together in one dataset and do the segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do a groupby by order and calculate some basic metrics like max, min, mean and the like.\n",
    "# We have seen that a large part of the payments are unique, therefore they will coincide in most, but in the other cases\n",
    "# they will provide us with useful information about the client.\n",
    "\n",
    "aggregated_payments = payments_df.groupby('order_id').agg(\n",
    "    max_pay = ('payment_value', 'max'), \n",
    "    min_pay = ('payment_value', 'min'),\n",
    "    mean_pay = ('payment_value', 'mean'),\n",
    "    total_pay = ('payment_value', 'sum'),\n",
    "    max_seq = ('payment_sequential', 'max')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_payments[aggregated_payments[\"max_seq\"] == 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "payments_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated_payments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_with_payments = pd.merge(orders_df, aggregated_payments, on = 'order_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "orders_with_payments.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df(orders_with_payments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_with_payments.head(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_with_payments.set_index('order_id', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we convert the dates to the dates format since until now they were objects and they did not allow us to do any transformation\n",
    "\n",
    "orders_with_payments['order_purchase_timestamp'] =\\\n",
    "pd.to_datetime(orders_with_payments['order_purchase_timestamp'], format = '%Y-%m-%d')\n",
    "\n",
    "orders_with_payments['order_delivered_customer_date'] =\\\n",
    "pd.to_datetime(orders_with_payments['order_delivered_customer_date'], format = '%Y-%m-%d')\n",
    "\n",
    "orders_with_payments['order_estimated_delivery_date'] =\\\n",
    "pd.to_datetime(orders_with_payments['order_estimated_delivery_date'], format = '%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df(orders_with_payments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create useful variables by extracting the order information.\n",
    "# For example: time since the last purchase can be very useful for our client to detect recent customers \n",
    "# and combined with the total orders variable it can be very valuable to segment very loyal customers\n",
    "\n",
    "orders_with_payments['last_purchase'] = orders_with_payments['order_purchase_timestamp'].max()\n",
    "\n",
    "orders_with_payments['time_since_last_purchase'] =\\\n",
    "orders_with_payments['last_purchase'] - orders_with_payments['order_purchase_timestamp']\n",
    "\n",
    "orders_with_payments['delivery_time'] =\\\n",
    "orders_with_payments['order_delivered_customer_date'] - orders_with_payments['order_purchase_timestamp']\n",
    "\n",
    "orders_with_payments['delay'] =\\\n",
    "orders_with_payments['order_delivered_customer_date'] - orders_with_payments['order_estimated_delivery_date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orders_with_payments.head().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"join\"></a>\n",
    "## Final Dataset Creation\n",
    "[Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(customer_df, orders_with_payments, on = \"customer_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.set_index(\"customer_unique_id\", inplace = True)\n",
    "\n",
    "lc = [\n",
    "    'max_pay',\n",
    "    'min_pay',\n",
    "    'mean_pay',\n",
    "    'total_pay',\n",
    "    'max_seq',\n",
    "    'time_since_last_purchase',\n",
    "    'delivery_time',\n",
    "    'delay'\n",
    "]\n",
    "\n",
    "df_final = df_final[lc]\n",
    "\n",
    "# We extract the days from the timedelta variable that we had previously calculated.\n",
    "df_final[\"time_since_last_purchase\"] = df_final[\"time_since_last_purchase\"].dt.days\n",
    "df_final[\"delivery_time\"] = df_final[\"delivery_time\"].dt.days\n",
    "df_final[\"delay\"] = df_final[\"delay\"].dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df(df_final)\n",
    "\n",
    "df_final.sample(3).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have null values and we will have to impute them correctly in our pipeline.\n",
    "df_final.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"skpipeline\"></a>\n",
    "## Pipeline Creation\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "At this point we have to do the last transformations of our dataframe as well as create the last variables for the **RFM model.**\n",
    "\n",
    "One way to do this is to use the sklearn pipeline to automate all these steps, but for this we have to implement our own ***\"Transformers.\"***\n",
    "\n",
    "A ***Transformer*** in sklearn is not just another class that has the **fit, transform and fit_transform** method implemented (which can perform any transformation on the dataset that you pass to it).\n",
    "\n",
    "However, in practice we are only going to implement one method, the **transform**, because the **fit** (when we inherit from ***TransformerMixin***) only has to return the ***self*** and the **fit_transform** is to be created only.\n",
    "\n",
    "We also inherit from the ***BaseEstimator*** to have access to the method of ***get_params() and set_params()*** although in practice we are not going to use it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_final_backup = df_final.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ***outliers*** can distort our KMeans a lot, being able to create groups of only 1 client.\n",
    "\n",
    "To prevent this from happening, we have to remove any possible ***outlier.*** and then standardize or normalize our data.\n",
    "\n",
    "Next we are going to implement our own ***OutlierFilter***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OutlierFilter(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    Class that filters outliers using np.quantile ()\n",
    "    The quantiles to filter as well as the columns to filter are the parameters of the class.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, q, col_to_filter):\n",
    "        self.q = q\n",
    "        self.col_to_filter = col_to_filter\n",
    "        \n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y = None):\n",
    "        '''\n",
    "        The method considers that client outlier that is outlier in all the columns that you pass to it.\n",
    "        That is to say: if you have to filter the amount and number of orders, you will only eliminate those customers\n",
    "        which are outlier both in amount and number of orders. If you are outlier in amount but not in order\n",
    "        it will not be filtered from the dataset.\n",
    "        '''\n",
    "        \n",
    "        # empty list\n",
    "        criteria_list = []\n",
    "        \n",
    "        # we add clients that are outliers to the list\n",
    "        for col in self.col_to_filter:\n",
    "            criteria = X[col] < np.quantile(X[col], q = self.q)\n",
    "            criteria_list.append(criteria)\n",
    "            \n",
    "        # if there is more than 1 column\n",
    "        if len(self.col_to_filter) > 1:\n",
    "            \n",
    "            # we create the global criterion: that is, outlier in all columns\n",
    "            global_criteria = criteria_list[0]\n",
    "            \n",
    "            for criteria in criteria_list[1:]:\n",
    "                global_criteria = global_criteria & criteria\n",
    "                \n",
    "        else:\n",
    "            global_criteria = criteria_list[0]\n",
    "            \n",
    "        # we filter our dataframe\n",
    "        X = X[global_criteria]\n",
    "        \n",
    "        # we save the index as a class parameter because otherwise we would lose it.\n",
    "        self.index = X.index\n",
    "        \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reached this point and we have everything ready to build our pipeline.\n",
    "\n",
    "The steps you are going to perform are:\n",
    "\n",
    "1. Use the ***KNNImputer***, which is based on the same notion of ** distance metrics ** to impute null values ​​based on the most similar clients.\n",
    "\n",
    "2. Since it returns an array in the first step, we use our own ***Transformer*** to convert to DataFrame.\n",
    "\n",
    "3. We create variables at the customer level. **SO OUR OUTPUT DATAFRAME WILL BE SMALLER.**\n",
    "\n",
    "4. We filter the outliers with our own ***Transformer***. **SO OUR OUTPUT DATAFRAME WILL BE SMALLER.**\n",
    "\n",
    "5. We standardize the values, using ***StandardScaler***.\n",
    "\n",
    "6. We make a fit with KMeans to calculate the **inertia** of the groups (the dispersion of the data to the centroid).\n",
    "\n",
    "***Elbow Curve*** Technique: we do all this in a loop because we want to see when there is a sudden change in inertia and therefore **increasing the number of centroids further does not pay off because the marginal gain it is very small.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALCULATE_ELBOW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separamos el pipeline del a loop, para no tener que volver a hacer los primeros 5 pasos para cada k de la loop\n",
    "pipe = Pipeline(steps = [\n",
    "    (\"Imputer\", KNNImputer()),\n",
    "    (\"ArrayToDataFrame\", ArrayToDataFrame(columns, index = index)),\n",
    "    (\"FeatureGenerator\", FeatureGenerator()),\n",
    "    (\"OutlierFilter\", OutlierFilter(q = 0.99, col_to_filter = [\"amount\", \"max_delay\"])),\n",
    "    (\"StandardScaler\", StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled_transformed_no_outliers = pipe.fit_transform(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"elbow_curve\"></a>\n",
    "# Elbow Curve\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "In this section we are going to visualize our ***Elbow Curve*** and we will look for the inflection point that will be our number of centroids."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_final = pd.read_csv\n",
    "df_final.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CALCULATE_ELBOW:\n",
    "    fig = plt.figure(figsize = (16, 8))\n",
    "    ax = fig.add_subplot()\n",
    "\n",
    "    x_values = list(sse.keys())\n",
    "    y_values = list(sse.values())\n",
    "\n",
    "    ax.plot(x_values, y_values, label = \"Inertia / dispersion of clusters\")\n",
    "    fig.suptitle(\"Variation of the dispersion of the clusters as a function of the k\", fontsize = 16);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"segmentation\"></a>\n",
    "## Customer Segmentation with the \"appropriate k\"\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "Now that we have determined the correct number of centroids we can fit our pipeline with the appropriate ***k.***\n",
    "\n",
    "Since we are going to carry out our segmentation with KMeans and we are going to supply them with the variables of our interest, sometimes, KMeans is known as **unsupervised but guided segmentation***. Guided because somehow the data scientist tells (guides) it to discriminate using some variables and not others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(steps = [\n",
    "    (\"Imputer\", KNNImputer()), \n",
    "    (\"ArrayToDataFrame\", ArrayToDataFrame(columns, index = index)),\n",
    "    (\"FeatureGenerator\", FeatureGenerator()),\n",
    "    (\"OutlierFilter\", OutlierFilter(q = 0.99, col_to_filter = [\"amount\", \"max_delay\"])),\n",
    "    (\"StandardScaler\", StandardScaler()),\n",
    "    (\"Clustering\", KMeans(n_clusters = 5))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(df_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A very interesting part of pipelines is that we can filter it (just like a python list) and use only part of the steps that we have implemented.\n",
    "\n",
    "This is very useful because in our case, when we are going to predict (assign each client its centroid), we want to impute the nulls, create the necessary variables and standardize (step 1, 3 and 5 of the pipe), but **not filter outliers** (all clients must have a group). If we predict with the entire pipeline, some clients will not be assigned to any group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create a scaled dataframe with steps 1, 3 and 5\n",
    "X_processed = pipe[:3].transform(df_final)\n",
    "X_scaled = pipe[\"StandardScaler\"].transform(X_processed)\n",
    "\n",
    "# We lose customers by step 3: customers with more than 1 order or payment, it ends up being grouped in 1 single record.\n",
    "X_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We do the predict, in this case we will have their centroid / cluster for each client.\n",
    "labels = pipe[\"Clustering\"].predict(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe[\"Clustering\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we assign the centroids to the processed DataFrame.\n",
    "# IF WE DO IT TO SCALING THE NUMBERS WILL LOSE THEIR MEANING TO SCALE AND WILL BE MORE DIFFICULT TO INTERPRET.\n",
    "X_processed[\"cluster\"] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We visualize our groups based on the variables of the RFM model, to see how they have been.\n",
    "selected_columns = ['n_orders', 'amount', 'last_purchase']\n",
    "\n",
    "sns.pairplot(X_processed, vars = selected_columns, hue = 'cluster');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"rfm\"></a>\n",
    "## RFM Model\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "In this section we are going to see how a segmentation based on 3 key indicators of: ***retention, frequency and monetary value (RFM) can be implemented very easily.***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we calculate the RFM indicators\n",
    "QUANTILES = 5\n",
    "X_processed['recency'] = pd.qcut(X_processed['last_purchase'], q = QUANTILES, labels = range(QUANTILES))\n",
    "X_processed['frequency'] = (X_processed['n_orders'] > 1).astype(int)\n",
    "X_processed['monetary_value'] = pd.qcut(X_processed['amount'], q = QUANTILES, labels = range(QUANTILES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed.pivot_table(index = 'recency', values = ['last_purchase'], aggfunc = [len, np.mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed.pivot_table(index = 'monetary_value', values = ['amount'], aggfunc = [len, np.mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed.pivot_table(index = 'frequency', values = ['n_orders'], aggfunc = [len, np.mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_df = pd.crosstab(X_processed['recency'], X_processed['monetary_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rm_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(rm_df, cmap = 'RdYlGn', linewidths = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_processed[['recency', 'frequency', 'monetary_value', \"n_orders\"]].\\\n",
    "groupby(['recency', 'frequency', 'monetary_value']).agg('count')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"summary\"></a>\n",
    "# Summary\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "The last step, used once we have done our complete segmentation, is to create a ***summary tab*** of each group with the main business variables or with those that have not been used in the segmentation to periodically monitor the groups or to send as a document to the rest of the company's departments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = pd.DataFrame()\n",
    "summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, col in enumerate([\"amount\", \"n_orders\", \"last_purchase\", \"mean_delay\"]):\n",
    "    summary_data = X_processed[[\"cluster\", col]].groupby(\"cluster\").describe().T[1:]\n",
    "    summary_df = summary_df.append(summary_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.head(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we generate our multiindex\n",
    "out_index = [\n",
    "     \"Monetary\",\n",
    "     \"Loyalty\",\n",
    "     \"Loyalty\",\n",
    "     \"Logistics\"\n",
    "]\n",
    "\n",
    "inner_index = [\n",
    "     \"Amount\",\n",
    "     \"Nr. Of purchases\",\n",
    "     \"Last purchase\",\n",
    "     \"Delays\"\n",
    "]\n",
    "\n",
    "statistics = [\"Average\", \"Deviation\", \"Minimum\", \"Perc. 25\", \"Perc. 50\", \"Perc. 75\", \"Maximum\"]\n",
    "\n",
    "new_multi_index = []\n",
    "\n",
    "for oi, ii, in zip(out_index, inner_index):\n",
    "    for es in statistics:\n",
    "        new_multi_index.append((oi, ii, es))\n",
    "        \n",
    "new_multi_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_multiindex(list_of_tuples, names):\n",
    "    return pd.MultiIndex.from_tuples(list_of_tuples, names = names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"Indicator Group\", \"Indicator\", \"Statistic\"]\n",
    "index_df = generate_multiindex(new_multi_index, names)\n",
    "summary_df.set_index(index_df, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_clusters = X_processed.groupby(\"cluster\").size().to_frame().T\n",
    "size_clusters.set_index(generate_multiindex([(\"General\", \"Cluster\", \"Size\")] , names), inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df = size_clusters.append(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"conclusion\"></a>\n",
    "# Conclusion\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "Unsupervised algorithms are **very powerful** tools that any data scientist should have on hand. Knowing how they work and implementing them correctly can allow you to **extract valuable information and make better business decisions**. We have seen different algorithms and unsupervised techniques (***KMeans and RFM model***) that allow us to create **homogeneous and actionable groups** of clients and thus improve the company's indicators.\n",
    "\n",
    "In addition to this, we have learned to use sklearn pipelines to **fully automate data processing** as well as implement our own ***Transformers*** that can be used within the pipeline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
