{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center > <span style = \"background-color: rgb(251, 160, 38); font-size: 32px;\" > <b > NUCLIO DIGITAL SCHOOL - </b > MASTER EN DATA SCIENCE < /span > </center >\n",
    "\n",
    "<br >\n",
    "\n",
    "<center > <a href = https: // nuclio.school/wp-content/uploads/2019/10/nucleoDS-newBlack.png > <img src = \"https://nuclio.school/wp-content/uploads/2019/10/nucleoDS-newBlack.png\" width = 400 height = 100 > <a/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">\n",
    "<center> <span style=\"font-size: 26px;\"> Machine Learning for Time Series </span> </center>\n",
    "\n",
    "<span style=\"font-size: 16px;\">\n",
    "\n",
    "+ Session: **Machine Learning for Time Series**\n",
    "+ Module: **Time Series Forecasting**\n",
    "+ Course: **Data Science Master 0921**\n",
    "+ Professor: **Christa Santos**\n",
    "\n",
    "</span>\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will prepare the data for machine learning analysis by creating lagged variables. We will work with both stationary and non-stationary data. Three of the datasets that we are using are seasonally adjusted for stationarity through differencing. The two financial datasets are only resampled from daily to monthly data.  We will create kernel density and autocorrelation plots as well as generate lag variables for a 12-time step period. We will then save these new dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import read_csv\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the vacation dataset from the previous modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data as a panda series\n",
    "# https://trends.google.com/trends/?geo=US , google trends, search the word \"vacation\"\n",
    "# Recall that data is from 2004 to 2019\n",
    "vacation = pd.read_csv(\"diff_data/vacation_firstdiff.csv\", index_col=0, parse_dates=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display first few rows\n",
    "print(vacation.head(5))\n",
    "# line plot of dataset\n",
    "vacation.plot(figsize=(8, 5))\n",
    "plt.show()\n",
    "# data is monthly and is made stationary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot shows that seasonality and trend were removed from the series through differening. Above is the plot of the differenced data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacation.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vacation.plot(kind='kde')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "# Check for autocorrelation of each lagged observation and whether it is statistically significant.\n",
    "plot_acf(vacation)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The blue shaded region is the margin of uncertainty. Candlesticks that extend out beyond the blue shaded region are considered statistically significant.  Correlation values are between 1 and -1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Lagged Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reframe as supervised learning\n",
    "# lag observation (t-1) is the input variable and t is the output variable.\n",
    "df1 = pd.DataFrame()\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 12 months of lag values to predict current observation\n",
    "# Shift of 12 months\n",
    "for i in range(12, 0, -1):\n",
    "    df1[['t-'+str(i)]] = vacation.shift(i)\n",
    "print(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create column t\n",
    "df1['t'] = vacation.values\n",
    "print(df1.head(13))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new subsetted dataframe, removing Nans from first 12 rows\n",
    "df1_vacat = df1[13:]\n",
    "print(df1_vacat)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to new file\n",
    "df1_vacat.to_csv('vacation_lags_12months_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeat the same for the furniture data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "furniture = pd.read_csv(\"diff_data/furn_pctchange.csv\", index_col=0, parse_dates=True)\n",
    "furniture.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision Tree Model for Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df1 = pd.read_csv('vacation_lags_12months_features.csv', header=0)\n",
    "df1.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data\n",
    "\n",
    "vacat = df1.values\n",
    "# split into lagged variables and original time series\n",
    "# slice all rows and start with column 0 and go up to but not including the last column\n",
    "X1 = vacat[:, 0:-1]\n",
    "# slice all rows and last column, essentially separating out 't' column\n",
    "y1 = vacat[:, -1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns t-1 to t-12, which are the lagged variables\n",
    "X1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Column t, which is the original time series\n",
    "y1[0:10]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Target Train-Test split\n",
    "from pandas import read_csv\n",
    "\n",
    "Y1 = y1\n",
    "traintarget_size = int(len(Y1) * 0.80)   # Set split\n",
    "train_target, test_target = Y1[0:traintarget_size], Y1[traintarget_size:len(Y1)]\n",
    "\n",
    "print('Observations for Target: %d' % (len(Y1)))\n",
    "print('Training Observations for Target: %d' % (len(train_target)))\n",
    "print('Testing Observations for Target: %d' % (len(test_target)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Train-Test split\n",
    "\n",
    "trainfeature_size = int(len(X1) * 0.80)\n",
    "train_feature, test_feature = X1[0:trainfeature_size], X1[trainfeature_size:len(X1)]\n",
    "print('Observations for feature: %d' % (len(X1)))\n",
    "print('Training Observations for feature: %d' % (len(train_feature)))\n",
    "print('Testing Observations for feature: %d' % (len(test_feature)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Regresion Model\n",
    "\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Create a decision tree regression model with default arguments\n",
    "decision_tree_vacat = DecisionTreeRegressor()  # max-depth not set\n",
    "\n",
    "# Fit the model to the training features and targets\n",
    "decision_tree_vacat.fit(train_feature, train_target)\n",
    "\n",
    "# Check the score on train and test\n",
    "print(decision_tree_vacat.score(train_feature, train_target))\n",
    "# predictions are horrible if negative value, no relationship if 0\n",
    "print(decision_tree_vacat.score(test_feature, test_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best Max Depth\n",
    "\n",
    "# Loop through a few different max depths and check the performance\n",
    "# Try different max depths. We want to optimize our ML models to make the best predictions possible.\n",
    "# For regular decision trees, max_depth, which is a hyperparameter, limits the number of splits in a tree.\n",
    "# You can find the best value of max_depth based on the R-squared score of the model on the test set.\n",
    "\n",
    "for d in [2, 3, 4, 5, 7, 8, 10]:\n",
    "    # Create the tree and fit it\n",
    "    decision_tree_vacat = DecisionTreeRegressor(max_depth=d)\n",
    "    decision_tree_vacat.fit(train_feature, train_target)\n",
    "\n",
    "    # Print out the scores on train and test\n",
    "    print('max_depth=', str(d))\n",
    "    print(decision_tree_vacat.score(train_feature, train_target))\n",
    "    # You want the test score to be positive and high\n",
    "    print(decision_tree_vacat.score(test_feature, test_target), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision-tree learners can create over-complex trees that do not generalize the data well. This is called overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot predicted against actual values\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Use the best max_depth\n",
    "decision_tree_vacat = DecisionTreeRegressor(\n",
    "    max_depth=8)  # fill in best max depth here\n",
    "decision_tree_vacat.fit(train_feature, train_target)\n",
    "\n",
    "# Predict values for train and test\n",
    "train_prediction = decision_tree_vacat.predict(train_feature)\n",
    "test_prediction = decision_tree_vacat.predict(test_feature)\n",
    "\n",
    "# Scatter the predictions vs actual values\n",
    "plt.scatter(train_prediction, train_target, label='train')  # blue\n",
    "plt.scatter(test_prediction, test_target, label='test')  # orange\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest for Time Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Create the random forest model and fit to the training data\n",
    "# n_estimators is the number of trees in forest\n",
    "rfr = RandomForestRegressor(n_estimators=200)\n",
    "rfr.fit(train_feature, train_target)\n",
    "\n",
    "# Look at the R^2 scores on train and test\n",
    "print(rfr.score(train_feature, train_target))\n",
    "print(rfr.score(test_feature, test_target))  # Try to attain a positive value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: This cell will take some time to run\n",
    "#  https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "import numpy as np\n",
    "\n",
    "# Create a dictionary of hyperparameters to search\n",
    "# n_estimators is the number of trees in the forest. The larger the better,\n",
    "# but also the longer it will take to compute.\n",
    "# max_features is the number of features chosen at random at splits\n",
    "# max_depth is the number of splits\n",
    "# random_state will make your result reproducible\n",
    "\n",
    "# Run grid search\n",
    "# grid = {'n_estimators': [200], 'max_depth': [2, 3, 4, 5, 6, 7, 8, 9, 10],'max_features': [2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12], 'random_state': [17]}\n",
    "grid = {'n_estimators': [200], 'max_depth': [10],\n",
    "        'max_features': [2], 'random_state': [17]}\n",
    "test_scores = []\n",
    "\n",
    "# Loop through the parameter grid, set the hyperparameters, and save the scores\n",
    "for g in ParameterGrid(grid):\n",
    "    rfr.set_params(**g)  # ** is \"unpacking\" the dictionary\n",
    "    rfr.fit(train_feature, train_target)\n",
    "    test_scores.append(rfr.score(test_feature, test_target))\n",
    "\n",
    "# Find best hyperparameters from the test score and print\n",
    "best_idx = np.argmax(test_scores)\n",
    "print(test_scores[best_idx], ParameterGrid(grid)[\n",
    "      best_idx])  # You don't want negative value\n",
    "\n",
    "# The best test score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the best hyperparameters from before to fit a random forest model\n",
    "rfr = RandomForestRegressor(\n",
    "    n_estimators=200, max_depth=10, max_features=2, random_state=17)\n",
    "rfr.fit(train_feature, train_target)\n",
    "\n",
    "# Make predictions with our model\n",
    "train_prediction = rfr.predict(train_feature)\n",
    "test_prediction = rfr.predict(test_feature)\n",
    "\n",
    "# Create a scatter plot with train and test actual vs predictions\n",
    "plt.scatter(train_target, train_prediction, label='train')\n",
    "plt.scatter(test_target, test_prediction, label='test')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Model Feature Importance\n",
    "\n",
    "# get column names\n",
    "df1.columns\n",
    "# Get feature importances from our random forest model\n",
    "importances = rfr.feature_importances_\n",
    "\n",
    "# Get the index of importances from greatest importance to least\n",
    "sorted_index = np.argsort(importances)[::-1]\n",
    "x1 = range(len(importances))\n",
    "\n",
    "# Create tick labels\n",
    "feature_names = ['t-12', 't-11', 't-10', 't-9', 't-8', 't-7', 't-6', 't-5', 't-4', 't-3',\n",
    "                 't-2', 't-1']\n",
    "labels = np.array(feature_names)[sorted_index]\n",
    "plt.bar(x1, importances[sorted_index], tick_label=labels)\n",
    "\n",
    "# Rotate tick labels to vertical\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosted Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.GradientBoostingRegressor.html\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "\n",
    "# The fraction of samples to be used for fitting the individual base learners.\n",
    "# Choosing subsample < 1.0 leads to a reduction of variance and an increase in bias.\n",
    "# Create GB model -- hyperparameters\n",
    "gbr = GradientBoostingRegressor(max_features=2,\n",
    "                                learning_rate=0.01,\n",
    "                                n_estimators=500,\n",
    "                                subsample=0.6,\n",
    "                                random_state=99)\n",
    "\n",
    "gbr.fit(train_feature, train_target)\n",
    "\n",
    "print(gbr.score(train_feature, train_target))\n",
    "print(gbr.score(test_feature, test_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gradient Boosted Model Feature Importance\n",
    "# Extract feature importances from the fitted gradient boosting model\n",
    "feature_importances = gbr.feature_importances_\n",
    "\n",
    "# Get the indices of the largest to smallest feature importances\n",
    "sorted_index = np.argsort(feature_importances)[::-1]\n",
    "x1 = range(X1.shape[1])\n",
    "\n",
    "# Create tick labels\n",
    "feature_names = ['t-12', 't-11', 't-10', 't-9', 't-8', 't-7', 't-6', 't-5', 't-4', 't-3',\n",
    "                 't-2', 't-1']\n",
    "labels = np.array(feature_names)[sorted_index]\n",
    "\n",
    "plt.bar(x1, feature_importances[sorted_index], tick_label=labels)\n",
    "\n",
    "# Set the tick lables to be the feature names, according to the sorted feature_idx\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0820050dde91fc2897eae9036e8ba4d22e350a3297f4d4b49abc97f4c24f3b88"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
