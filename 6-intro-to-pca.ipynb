{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><span style=\"background-color: rgb(251, 160, 38); font-size: 32px;\">  <b>NUCLIO DIGITAL SCHOOL -</b> MASTER EN DATA SCIENCE  </span></center>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center><a href = https://nuclio.school/wp-content/uploads/2019/10/nucleoDS-newBlack.png > <img src=\"https://nuclio.school/wp-content/uploads/2019/10/nucleoDS-newBlack.png\" width=400 height=100><a/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">\n",
    "    <center> <span style=\"font-size: 26px;\"> Principal Component Analysis </span> </center>\n",
    "\n",
    "+ Session: **Principal Component Analysis**\n",
    "+ Module: **Unsupervised Learning**\n",
    "+ Course: **Data Science Master 0921**\n",
    "+ Professor: **Christa Santos**\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduccion\n",
    "\n",
    "We read and understand the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris = pd.read_csv('https://gist.githubusercontent.com/netj/8836201/raw/6f9306ad21398ea43cba4f7d537619d0e07d5ae3/iris.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_iris.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter_matrix(df_iris, height = 800)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['sepal.length','sepal.width','petal.length','petal.width']\n",
    "heatmap = sns.heatmap(df_iris[features].corr(), vmin=-1, vmax=1, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardize the Data\n",
    "\n",
    "The PCA calculates a new projection from your data set. And the new axes are based on the standard deviation of their variables. So a variable with a high standard deviation will have a higher weight for the axis calculation than a variable with a low standard deviation. If you standardize your data, all variables have the same standard deviation, therefore all variables have the same weight and your PCA calculates the relevant axes.\n",
    "\n",
    "Example: A dataset has variables with different units, such as one in KM and one in CM (centimeter), but both have the same change in value, so the variable in KM will reflect a lesser change, while the other will have some changes of greater value. In this case, if we do not standardize the variable, PCA will give greater preference to the variable in centimeters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['sepal.length', 'sepal.width', 'petal.length', 'petal.width']\n",
    "\n",
    "x = df_iris.loc[:, features].values\n",
    "\n",
    "display(px.histogram(x, nbins = 50,marginal=\"box\"))\n",
    "x = StandardScaler().fit_transform(x)\n",
    "df_x = pd.DataFrame(x)\n",
    "px.histogram(df_x, nbins = 100, marginal=\"box\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the PCA\n",
    "\n",
    "The sklearn.decomposition.PCA class incorporates the main functionalities that are needed when working with PCA models. The n_components argument determines the number of calculated components. If None is specified, all possible ones are calculated (min (rows, columns) - 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "\n",
    "principalComponents = pca.fit_transform(x)\n",
    "\n",
    "principalDataframe = pd.DataFrame(data = principalComponents, columns = ['PC1', 'PC2','PC3','PC4'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze the results (Select the number)\n",
    "\n",
    "Once the PCA object has been trained, all the information of the created components can be accessed.\n",
    "\n",
    "\n",
    "One of the most frequent questions that arises after performing a PCA is: How much information present in the original data set is lost when projecting the observations in a space of less dimension? or what is the same, how much information is capable of capturing each of the main components obtained? To answer these questions, the proportion of variance explained by each principal component is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_variance = np.round(pca.explained_variance_ratio_* 100, decimals =2)\n",
    "columns = ['PC1', 'PC2', 'PC3', 'PC4']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(x = columns, y = percent_variance, text = percent_variance, template = 'plotly_white', title = 'Variance explained by each component')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.bar(x = columns, y = percent_variance.cumsum(), text = percent_variance.cumsum(), template = 'plotly_white', title = 'Cumulative variance explained by each component')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize and Interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.scatter(x = principalDataframe.PC1, y = principalDataframe.PC2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targetDataframe = df_iris[['variety']]\n",
    "\n",
    "newDataframe = pd.concat([principalDataframe, targetDataframe],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation\n",
    "\n",
    "components_ contains the value of the loadings ùúô that define each component (eigenvector). The rows correspond to the principal components (ordered from highest to lowest explained variance). The rows correspond to the input variables. Analyzing in detail the vector of loadings that each component forms can help to interpret what type of information each one of them collects. For example, the first component is the result of the following linear combination of the original variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The array is converted to dataframe to add names to the axes.\n",
    "pd.DataFrame(\n",
    "    data    = pca.components_,\n",
    "    columns = features,\n",
    "    index   = ['PC1', 'PC2', 'PC3', 'PC4']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_\n",
    "pca.explained_variance_\n",
    "pca.explained_variance_ratio_\n",
    "pca.singular_values_\n",
    "pca.mean_\n",
    "pca.n_components_\n",
    "pca.n_features_\n",
    "pca.n_samples_\n",
    "pca.noise_variance_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New sample transformation\n",
    "\n",
    "Once the model has been trained, with the transform () method, the dimensionality of new observations can be reduced by projecting them into the space defined by the components."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.transform(principalDataframe)[0:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruction backwards\n",
    "\n",
    "You can reverse the transformation and rebuild the initial value with the inverse_transform() method. It is important to note that the rebuild will only be complete if all components have been included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(1)\n",
    "X_orig = np.random.rand(10, 2)\n",
    "X_re_orig = pca.inverse_transform(pca.fit_transform(X_orig))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "plt.scatter(X_orig[:, 0], X_orig[:, 1], label='Original points')\n",
    "plt.scatter(X_re_orig[:, 0], X_re_orig[:, 1], label='InverseTransform')\n",
    "[plt.plot([X_orig[i, 0], X_re_orig[i, 0]], [X_orig[i, 1], X_re_orig[i, 1]]) for i in range(10)]\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA from 0 (Mathematical Explanation)\n",
    "\n",
    "Although we are not going to see it in depth, it is necessary to know what the PCA is based on mathematically. We are going to see the calculation based on the covariance method, based on the covariance matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing required libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate un dataset\n",
    "X = np.random.randint(10,50,100).reshape(20,5) \n",
    "# mean Centering the data  \n",
    "X_meaned = X - np.mean(X , axis = 0)\n",
    "X_meaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The covariance matrix centered on the mean is calculated\n",
    "cov_mat = np.cov(X_meaned , rowvar = False)\n",
    "pd.DataFrame(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eigenvalues and Eigenvectors are calculated from the covariance matrix\n",
    "eigen_values , eigen_vectors = np.linalg.eigh(cov_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigen_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The eigen_values are sorted\n",
    "sorted_index = np.argsort(eigen_values)[::-1]\n",
    " \n",
    "sorted_eigenvalue = eigen_values[sorted_index]\n",
    "# Similarly sort the eigenvectors \n",
    "sorted_eigenvectors = eigen_vectors[:,sorted_index]\n",
    "sorted_eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The first eigenvectors are selected, depending on the number of components desired of our final reduced data.\n",
    " \n",
    "n_components = 2 # you can select any number of components.\n",
    "eigenvector_subset = sorted_eigenvectors[:,0:n_components]\n",
    "eigenvector_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And the transformation of the data is carried out\n",
    "X_reduced = np.dot(eigenvector_subset.transpose(),X_meaned.transpose()).transpose()\n",
    "X_reduced[0:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Your Turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_penguins = 'https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv'\n",
    "df_penguins = pd.read_csv(link_penguins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
