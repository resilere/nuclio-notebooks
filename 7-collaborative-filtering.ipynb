{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><span style=\"background-color: rgb(251, 160, 38); font-size: 32px;\">  <b>NUCLIO DIGITAL SCHOOL -</b> MASTER EN DATA SCIENCE  </span></center>\n",
    "    \n",
    "<br>\n",
    "    \n",
    "<center><a href = https://nuclio.school/wp-content/uploads/2019/10/nucleoDS-newBlack.png > <img src=\"https://nuclio.school/wp-content/uploads/2019/10/nucleoDS-newBlack.png\" width=400 height=100><a/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">\n",
    "    <center> <span style=\"font-size: 26px;\"> PCA and Collaborative Filtering </span> </center>\n",
    "\n",
    "+ Session: **PCA and Collaborative Filtering**\n",
    "+ Module: **Unsupervised Learning**\n",
    "+ Course: **Data Science Master 0921**\n",
    "+ Professor: **Christa Santos**\n",
    "\n",
    "<br>\n",
    "\n",
    "<hr style=\"height:5px;border-width:0;color:orange;background-color:orange\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"toc\"></a>\n",
    "# Table of Contents\n",
    "[1. Objective](#objectives)\n",
    "\n",
    "[2. Import the main libraries](#import_modules)\n",
    "\n",
    "[3. Import the data](#import_data)\n",
    "\n",
    "[4. Exploratory Data Analysis (EDA)](#eda)\n",
    "\n",
    "---> [EDA anime df](#df1)\n",
    "\n",
    "---> [EDA ratings df](#df2)\n",
    "\n",
    "[5. Join final animes with user ratings](#join)\n",
    "\n",
    "[6. Dimensionality Reduction with PCA](#pca)\n",
    "\n",
    "[7. Calculate similarity between users and animes reviews](#collaborative_filtering)\n",
    "\n",
    "[8. User based Recommendation](#recommendation_users)\n",
    "\n",
    "[9. Product based Recommendation](#recommendation_animes)\n",
    "\n",
    "[10. Conclusion](#conclusion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"objectives\"></a>\n",
    "## 1. Objective\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "One of the areas of machine learning with which we interact almost daily are the Collaborative Filtering models. In today's notebook, we will create 2 CF models: **one based on users (people similar to you, have bought / seen / liked these things) and another based on products (people who have bought this product, have also bought these).**\n",
    "\n",
    "The dataset that we are going to use is a Japanese Animes dataset and it can be downloaded at the following [link](https://www.kaggle.com/CooperUnion/anime-recommendations-database)\n",
    "\n",
    "Our main objectives will be:\n",
    "1. **Do an initial exploration of the two datasets** and understand the distribution of the data.\n",
    "\n",
    "2. **Extract some useful variables such as: anime genre** and eliminate users who have not rated the anime.\n",
    "\n",
    "3. **Reduce the dimensionality of our DataFrame using the PCA**\n",
    "\n",
    "4. **Segment our clients using the reduced dataset**\n",
    "\n",
    "5. **Use the similarity of the cosine to make recommendations to our customers (user and product based)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"import_modules\"></a>\n",
    "## 2. Import the main libraries\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "In this section of the kernel we are going to load the main libraries that we are going to use in our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# silence warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# operating system\n",
    "import os\n",
    "\n",
    "# time calculation to track some processes\n",
    "import time\n",
    "\n",
    "# numeric and matrix operations\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# scientific computations library\n",
    "import scipy as sp\n",
    "\n",
    "# loading ploting libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('ggplot')\n",
    "%matplotlib inline\n",
    "\n",
    "# import the function to compute cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "PATH_FOLDER = os.path.join(os.getcwd(), \"data\\\\anime_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_FOLDER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"import_data\"></a>\n",
    "## 3. Import the data\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "In this section of the kernel we are going to load the main datasets that we are going to use to build our recommender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_ANIME = os.path.join(PATH_FOLDER, 'anime.csv')\n",
    "\n",
    "anime_df = pd.read_csv(PATH_ANIME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_RATING = os.path.join(PATH_FOLDER, 'rating.csv')\n",
    "\n",
    "rating_df = pd.read_csv(PATH_RATING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE_TO_IMPUTE = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"eda\"></a>\n",
    "## 4. Exploratory Data Analysis (EDA)\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "In the EDA section we will make **a first approximation to our data** to see its composition and what variables we have at our disposal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"df1\"></a>\n",
    "### 4.1 Anime dataset\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "Quick EDA on **anime dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def report_df(df, verbose = True):\n",
    "    '''\n",
    "    Makes a simple report on the supplied DataFrame.\n",
    "    '''\n",
    "    print(df.info(verbose = verbose))\n",
    "    total_nulos = df.isnull().sum().sum()\n",
    "    print()\n",
    "    print(f\"Tenemos un total de {total_nulos} nulos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df(anime_df)\n",
    "\n",
    "anime_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We note that we have some nulls and we will have to deal with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't have duplicate anime_id's, as you would expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(anime_df[\"anime_id\"].value_counts() > 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By type of anime, we see that the most popular is the one on TV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df[\"type\"].value_counts().plot(kind = \"bar\", title = \"Animes by type\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our report_df we have seen that ** episodes ** seemed to be numeric, but it could contain other types of data (because it is object), we convert this column to a number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df[\"episodes\"] = pd.to_numeric(anime_df[\"episodes\"], errors = \"coerce\")\n",
    "anime_df[\"episodes\"].fillna(1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next section we are going to analyze the distribution of the animes based on the number of episodes they have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_episodes = anime_df[\"episodes\"].value_counts().to_frame().reset_index()\n",
    "count_per_episodes.columns = [\"nr_episodes\", \"nr_films\"]\n",
    "count_per_episodes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_episodes.sort_values(\"nr_episodes\", ascending = True, inplace = True)\n",
    "count_per_episodes[\"pct_over_total\"] = count_per_episodes[\"nr_films\"]/count_per_episodes[\"nr_films\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_episodes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Almost half of the anime is a single episode (48.94%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NR = 30\n",
    "\n",
    "# instanciate the figure\n",
    "fig = plt.figure(figsize = (15, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# separete the data\n",
    "x = count_per_episodes[\"nr_episodes\"].values[:NR]\n",
    "y = count_per_episodes[\"nr_films\"].values[:NR]\n",
    "y_pct = count_per_episodes[\"pct_over_total\"].values[:NR]\n",
    "\n",
    "# plot the data\n",
    "barplot = ax.bar(x, y)\n",
    "\n",
    "# add text to each column\n",
    "for rect, y_pct_ in zip(barplot, y_pct):\n",
    "    y_pct_ = round(y_pct_*100, 2)\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2.0, height, f\"{height}:{y_pct_}%\", ha = 'center', va = \"bottom\", rotation = 60)\n",
    "\n",
    "# change the xticks\n",
    "ax.set_xticks(np.arange(0, NR + 1))\n",
    "\n",
    "# add title\n",
    "total_y_pct = round(sum(y_pct)*100, 2)\n",
    "ax.set_title(f\"Distribution of the first {NR} animes ({total_y_pct}% of the total)\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to carry out an analysis similar to the previous one, but now we will see how the animes are distributed based on the average score.\n",
    "\n",
    "To have only 10 groups, we are first going to round the mean score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df[\"rating\"].fillna(np.mean(anime_df[\"rating\"]), inplace = True)\n",
    "anime_df[\"ceil_rating\"] = anime_df[\"rating\"].apply(lambda rating: np.round(rating, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df[[\"ceil_rating\",\"rating\"]].groupby([\"ceil_rating\",\"rating\"]).size()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most common score is a 7 and it is found in 4,579 animes (37.25% of the total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_per_rating = anime_df[\"ceil_rating\"].value_counts().to_frame()\\\n",
    ".reset_index().sort_values(\"index\", ascending = True)\n",
    "\n",
    "# instanciate the figure\n",
    "fig = plt.figure(figsize = (15, 5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "# separete the data\n",
    "x = count_per_rating[\"index\"].values\n",
    "y = count_per_rating[\"ceil_rating\"].values\n",
    "y_pct = y/sum(y)\n",
    "\n",
    "# plot the data\n",
    "barplot = ax.bar(x, y)\n",
    "\n",
    "# add text to each column\n",
    "for rect, y_pct_ in zip(barplot, y_pct):\n",
    "    y_pct_ = round(y_pct_*100, 2)\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2.0, height, f\"{height} - {y_pct_}%\", ha = 'center', va = \"bottom\")\n",
    "    \n",
    "ax.set_xticks(np.arange(0, 11))\n",
    "ax.set_title(\"Distribution of the number of animes by rating\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will extract the genres of each anime and convert it into columns, which we can add to the initial dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df[\"genre\"].fillna(\"Unknown\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = anime_df[\"genre\"].str.split(\",\").to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll_with_id = []\n",
    "\n",
    "for ids, lists in zip(list(anime_df[\"anime_id\"].values), ll):\n",
    "    list_ = [ids]\n",
    "    for values in lists:\n",
    "        list_.append(values)\n",
    "    ll_with_id.append(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_by_genre = pd.DataFrame(ll_with_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_by_genre = pd.DataFrame(ll_with_id).melt(id_vars = 0)\n",
    "anime_by_genre[\"value\"].fillna(\"Unknown\", inplace = True)\n",
    "anime_by_genre.rename(columns = {0:\"anime_id\"}, inplace = True)\n",
    "anime_by_genre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_by_genre = anime_by_genre.pivot_table(index = \"anime_id\", columns = \"value\", aggfunc = len, fill_value = 0)\n",
    "anime_by_genre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_columns = [tupple[1] for tupple in list(anime_by_genre.columns)]\n",
    "new_columns = list(map(lambda text: text[1:] if text[0] == \" \" else text, new_columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_by_genre.columns = new_columns\n",
    "anime_by_genre.reset_index(inplace = True)\n",
    "anime_by_genre.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df.shape[0] == anime_by_genre.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# anime_df = pd.merge(anime_df, anime_by_genre, how = \"left\", on = [\"anime_id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "anime_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"df2\"></a>\n",
    "### 4.2 Ratings dataset\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "Quick EDA on **ratings dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df(rating_df)\n",
    "\n",
    "rating_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_gb = rating_df[\"rating\"].value_counts().reset_index().sort_values(\"index\", ascending = True)\n",
    "rating_gb.sort_values(\"index\", ascending = True, inplace = True)\n",
    "\n",
    "fig = plt.figure(figsize = (15, 5))\n",
    "ax = fig.add_subplot()\n",
    "\n",
    "x = rating_gb[\"index\"]\n",
    "y = rating_gb[\"rating\"]\n",
    "y_pct = y/sum(y)\n",
    "\n",
    "barplot = ax.bar(x, y)\n",
    "\n",
    "# add text to each column\n",
    "for rect, y_pct_ in zip(barplot, y_pct):\n",
    "    y_pct_ = round(y_pct_*100, 2)\n",
    "    height = rect.get_height()\n",
    "    plt.text(rect.get_x() + rect.get_width()/2.0, height, f\"{height} - {y_pct_}%\", ha = 'center', va = \"bottom\", rotation = 60)\n",
    "\n",
    "ax.set_xticks(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that we have **almost 20% of animes without reviews.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to see the users with the most reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_pivot = rating_df[\"user_id\"].value_counts()\n",
    "user_pivot.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User 48766 is clearly a \"weird\" user. He has more than 10k reviews, but they are all null.\n",
    "rating_df[rating_df[\"user_id\"] == 48766].shape[0] == -rating_df[rating_df[\"user_id\"] == 48766][\"rating\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rating_df[rating_df[\"user_id\"] == 48766].shape[0])\n",
    "print(rating_df[rating_df[\"user_id\"] == 48766][\"rating\"].sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to eliminate from our DataFrame all those users who have scored all -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = rating_df.groupby('user_id')['rating'].apply(set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_no_reviews = s.to_frame()[s.to_frame()[\"rating\"] == {-1}].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id_no_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df = rating_df[-rating_df[\"user_id\"].isin(user_id_no_reviews)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df[rating_df[\"user_id\"] == 2][\"rating\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"join\"></a>\n",
    "## 5. Join final animes with user ratings\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "Once we have analyzed our DataFrames, we are going to do a join **by anime_id.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rating_df.rename(columns = {\"rating\":\"user_rating\"}, inplace = True)\n",
    "anime_df.rename(columns = {\"rating\":\"average_rating\"}, inplace = True)\n",
    "df_final = pd.merge(rating_df, anime_df, on = \"anime_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We filter only by type == TV to have a more manageable dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final[\"user_rating\"].replace(-1, np.nan, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final[df_final[\"type\"] == \"TV\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = df_final.pivot_table(index = \"user_id\", columns = \"name\", values = \"user_rating\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VALUE_TO_IMPUTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VALUE_TO_IMPUTE != \"MEAN\":\n",
    "    df_final.fillna(VALUE_TO_IMPUTE, inplace = True)\n",
    "\n",
    "elif VALUE_TO_IMPUTE == \"MEAN\":\n",
    "    imputer = SimpleImputer(strategy = \"mean\")\n",
    "    X_imputed = imputer.fit_transform(df_final)\n",
    "    df_final = pd.DataFrame(X_imputed, index = df_final.index, columns = df_final.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"pca\"></a>\n",
    "## 6. Dimensionality Reduction with PCA\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "The PCA is the most widely used method of dimensionality reduction and with a long theoretical development behind it (it was [invented in 1901] (https://en.wikipedia.org/wiki/Principal_component_analysis) by Karl Pearson.\n",
    "\n",
    "The basic idea behind the PCA is the projection of a dataset with **n** dimensions to another **m** dimensions, where **n > m**, in such a way that the dataset in the new space preserves the maximum amount of information (variance) of the original space. Therefore, the new reduced dataset is a linear combination of the original **n** dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we initialize the sklearn PCA, we have to specify the number of components that we want our new dataset to have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = time.time()\n",
    "pca = PCA(n_components = 30)\n",
    "pca.fit(df_final)\n",
    "pca_samples = pca.transform(df_final)\n",
    "et = time.time()\n",
    "print(\"Total PCA took {} minutes\".format(round((et - st)/60, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative explained variance');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df = pd.DataFrame(pca_samples[:, 0:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you might expect, the projected dataset \"loses\" information when performing the transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to initialize the PCA, is instead of specifying the number of components, tell the PCA how much minimum variance the original dataset should contain (it has to be a number between 0 - 1).\n",
    "\n",
    "pca = PCA (n_components = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components = 0.7)\n",
    "pca"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"collaborative_filtering\"></a>\n",
    "## 7. Calculate similarity between users and animes reviews\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "In this section we are going to calculate the cosine similarity for 4000 customers (and the products they have evaluated)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CALCULATE_SIMILARITY = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CALCULATE_SIMILARITY:\n",
    "\n",
    "    st = time.time()\n",
    "    df_final_norm = df_final.apply(lambda x: (x - np.mean(x))/(np.max(x) - np.min(x)), axis = 1)\n",
    "    et = time.time()\n",
    "    print(\"Normalization took {} minutes\".format(round((et - st)/60, 2)))\n",
    "\n",
    "    df_final_norm = df_final_norm[df_final_norm.index < 3000]\n",
    "    sparse_ratings = sp.sparse.csr_matrix(df_final_norm.values)\n",
    "\n",
    "    st = time.time()\n",
    "    \n",
    "    item_similarity = cosine_similarity(sparse_ratings.T)\n",
    "    item_sim_df = pd.DataFrame(item_similarity, index = df_final_norm.columns, columns = df_final_norm.columns)\n",
    "    item_sim_df.to_pickle(os.path.join(PATH_FOLDER, \"item_similarity.pkl\"))\n",
    "\n",
    "    user_similarity = cosine_similarity(sparse_ratings)\n",
    "    user_sim_df = pd.DataFrame(user_similarity, index = df_final_norm.index, columns = df_final_norm.index)\n",
    "    user_sim_df.to_pickle(os.path.join(PATH_FOLDER, \"user_similarity.pkl\"))\n",
    "    \n",
    "    et = time.time()\n",
    "    print(\"Total time to calculate similarity took {} minutes.\".format(round((et - st)/60, 2)))\n",
    "    \n",
    "else:\n",
    "    user_sim_df = pd.read_pickle(os.path.join(PATH_FOLDER, \"user_similarity.pkl\"))\n",
    "    item_sim_df = pd.read_pickle(os.path.join(PATH_FOLDER, \"item_similarity.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_sim_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_sim_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"recommendation_users\"></a>\n",
    "## 8. User based Recommendation\n",
    "[Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_users(user, df):\n",
    "    '''\n",
    "    This function prints the top 10 similar users based on cosine similarity.\n",
    "    '''\n",
    "    \n",
    "    if user not in df.columns:\n",
    "        return('No data available on user {}'.format(user))\n",
    "    \n",
    "    print('Most Similar Users:\\n')\n",
    "    \n",
    "    sim_users = df.sort_values(by = user, ascending=False).index[1:11]\n",
    "    sim_values = df.sort_values(by = user, ascending=False).loc[:,user].tolist()[1:11]\n",
    "    \n",
    "    for user, sim in zip(sim_users, sim_values):\n",
    "        print('User #{0}, Similarity value: {1:.2f}'.format(user, sim)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_2_users(user1, user2, df, nr_animes):\n",
    "    '''\n",
    "    Returns a DataFrame with top 10 animes by 2 similar users (based on cosine similarity).\n",
    "    '''\n",
    "\n",
    "    top_10_user_1 = df[df.index == user1].melt().sort_values(\"value\", ascending = False)[:nr_animes]\n",
    "    top_10_user_1.columns = [\"name_user_{}\".format(user1), \"rating_user_{}\".format(user1)]\n",
    "    top_10_user_1 = top_10_user_1.reset_index(drop = True)\n",
    "\n",
    "    top_10_user_2 = df[df.index == user2].melt().sort_values(\"value\", ascending = False)[:nr_animes]\n",
    "    top_10_user_2.columns = [\"name_user_{}\".format(user2), \"rating_user_{}\".format(user2)]\n",
    "    top_10_user_2 = top_10_user_2.reset_index(drop = True)\n",
    "\n",
    "    combined_2_users = pd.concat([top_10_user_1, top_10_user_2], axis = 1, join = \"inner\")\n",
    "    \n",
    "    return combined_2_users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user1 = 20\n",
    "\n",
    "user2 = 2390\n",
    "\n",
    "top_users(user1, user_sim_df)\n",
    "\n",
    "combined_2_users = compare_2_users(user1, user2, df_final, 10)\n",
    "combined_2_users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"recommendation_animes\"></a>\n",
    "## 9. Product based Recommendation\n",
    "[Table of Contents](#toc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_animes(name, df):\n",
    "    '''\n",
    "    This functions prints top 10 similar animes, based on the reviews of the users.\n",
    "    '''\n",
    "    print('Similar shows to {} include:\\n'.format(name))\n",
    "    \n",
    "    index = item_sim_df[name].sort_values(ascending = False).index[1:11]\n",
    "    values = item_sim_df[name].sort_values(ascending = False).values[1:11]\n",
    "\n",
    "    for i, (index_, values_) in enumerate(zip(index, values)):\n",
    "        print('No. {}: {} ({})'.format(i + 1, index_, round(values_, 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_animes(\"07-Ghost\", item_sim_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id = \"conclusion\"></a>\n",
    "## 10. Conclusion\n",
    "[Table of Contents](#toc)\n",
    "\n",
    "In this Notebook we have explored some of the most common techniques used in unsupervised learning such as: **PCA, KMeans.**\n",
    "\n",
    "Subsequently, we have used the **\"cosine similarity\"** metric to create two collaborative filtering models: **user and product based.**\n",
    "\n",
    "We have seen how UL techniques are very useful and can be used in countless fields from: **data visualization, creation of new variables (the main components), reducing dimensionality to speed up learning, among others.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
